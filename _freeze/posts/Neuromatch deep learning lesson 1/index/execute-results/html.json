{
  "hash": "16446a0d66bfd7b38fd36f864a4f9ef2",
  "result": {
    "markdown": "---\ntitle: \"Basics of Pytorch\"\nauthor: \"Rhys McAlister\"\ndate: \"2022-10-16\"\ncategories: [news,analysis]\nimage: \"image.jpg\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Imports\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# PyTorch libraries\nimport torch\nfrom torch import nn\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\n```\n:::\n\n\nPytorch can be used for the direct construction of tensors in the following manner.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# From a list\na = torch.tensor([0,1,2])\n\n# From a tuple of tuples\nb = ((1.0,1.1), (1.2,1.3))\nb = torch.tensor(b)\n\n# From a numpy array\nc = np.ones([2,3])\nc = torch.tensor(c)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Tensor b: {b}\")\nprint(f\"Tensor c: {c}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensor a: tensor([0, 1, 2])\nTensor b: tensor([[1.0000, 1.1000],\n        [1.2000, 1.3000]])\nTensor c: tensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\n```\n:::\n:::\n\n\nIt is also possible to create tensors from various forms of random number distributions.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Uniform distribution\na = torch.rand(1, 3)\n\n# Normal distribution\nb = torch.randn(3, 4)\n\n# There are also constructors that allow us to construct\n# a tensor according to the above constructors, but with\n# dimensions equal to another tensor.\n\nc = torch.zeros_like(a)\nd = torch.rand_like(c)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Tensor b: {b}\")\nprint(f\"Tensor c: {c}\")\nprint(f\"Tensor d: {d}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensor a: tensor([[0.6242, 0.3680, 0.1774]])\nTensor b: tensor([[-1.4357, -0.3610, -0.4312, -1.1694],\n        [ 1.0250,  0.8717,  1.0414, -0.2386],\n        [-0.8637,  1.2593,  0.0024,  0.6281]])\nTensor c: tensor([[0., 0., 0.]])\nTensor d: tensor([[0.6016, 0.5186, 0.7333]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport torch\ntorch.manual_seed(0)\n\nimport random\nrandom.seed(0)\n\nimport numpy as np\nnp.random.seed(0)\n\nseed = 10\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness. NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef simplefun(seed=True, my_seed=None):\n  \"\"\"\n  Helper function to verify effectiveness of set_seed attribute\n\n  Args:\n    seed: Boolean\n      Specifies if seed value is provided or not\n    my_seed: Integer\n      Initializes seed to specified value\n\n  Returns:\n    Nothing\n  \"\"\"\n  if seed:\n    set_seed(seed=my_seed)\n\n  # uniform distribution\n  a = torch.rand(1, 3)\n  # normal distribution\n  b = torch.randn(3, 4)\n\n  print(\"Tensor a: \", a)\n  print(\"Tensor b: \", b)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsimplefun(seed=True, my_seed=0)  # Turn `seed` to `False` or change `my_seed`\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom seed 0 has been set.\nTensor a:  tensor([[0.4963, 0.7682, 0.0885]])\nTensor b:  tensor([[ 0.3643,  0.1344,  0.1642,  0.3058],\n        [ 0.2100,  0.9056,  0.6035,  0.8110],\n        [-0.0451,  0.8797,  1.0482, -0.0445]])\n```\n:::\n:::\n\n\nPytorch also provides equivalents of numpy .`linscape()` and `.arrange()` functions which operate as you would expect.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\na = torch.arange(0, 10, step=1)\nb = np.arange(0, 10, step=1)\n\nc = torch.linspace(0, 5, steps=11)\nd = np.linspace(0, 5, num=11)\n\nprint(f\"Tensor a: {a}\\n\")\nprint(f\"Numpy array b: {b}\\n\")\nprint(f\"Tensor c: {c}\\n\")\nprint(f\"Numpy array d: {d}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nNumpy array b: [0 1 2 3 4 5 6 7 8 9]\n\nTensor c: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n        4.5000, 5.0000])\n\nNumpy array d: [0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5. ]\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef tensor_creation(Z):\n  \"\"\"\n  A function that creates various tensors.\n\n  Args:\n    Z: numpy.ndarray\n      An array of shape (3,4)\n\n  Returns:\n    A : Tensor\n      20 by 21 tensor consisting of ones\n    B : Tensor\n      A tensor with elements equal to the elements of numpy array Z\n    C : Tensor\n      A tensor with the same number of elements as A but with values âˆ¼U(0,1)\n    D : Tensor\n      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n  \"\"\"\n  #################################################\n  ## TODO for students: fill in the missing code\n  ## from the first expression\n  \n  #################################################\n  A = torch.ones(20,21)\n  B = torch.tensor(Z)\n  C = torch.randn(20,21)\n  D = torch.linspace(4,41, steps=2)\n  \n  return A, B, C, D\n\n\n# numpy array to copy later\nZ = np.vander([1, 2, 3], 4)\n\n# Uncomment below to check your function!\nA, B, C, D = tensor_creation(Z)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\na = torch.ones(5, 3)\nb = torch.rand(5, 3)\nc = torch.empty(5, 3)\nd = torch.empty(5, 3)\n\n# this only works if c and d already exist\ntorch.add(a, b, out=c)\n\n# Pointwise Multiplication of a and b\ntorch.multiply(a, b, out=d)\n\nprint(c)\nprint(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[1.1074, 1.6594, 1.7684],\n        [1.5697, 1.1655, 1.1123],\n        [1.3457, 1.7195, 1.9932],\n        [1.7875, 1.4437, 1.6753],\n        [1.0095, 1.0729, 1.7333]])\ntensor([[0.1074, 0.6594, 0.7684],\n        [0.5697, 0.1655, 0.1123],\n        [0.3457, 0.7195, 0.9932],\n        [0.7875, 0.4437, 0.6753],\n        [0.0095, 0.0729, 0.7333]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n  \"\"\"\n  Helper function to demonstrate simple operations\n  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n\n  Args:\n    a1: Torch tensor\n      Tensor of size ([2,2])\n    a2: Torch tensor\n      Tensor of size ([2,2])\n    a3: Torch tensor\n      Tensor of size ([2,2])\n\n  Returns:\n    answer: Torch tensor\n      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n  \"\"\"\n  ################################################\n  ## TODO for students:  complete the first computation using the argument matricies\n  \n  ################################################\n  #\n  answer = a1 @ a2 + a3 \n  return answer\n\n# add timing to airtable\n\n\n# Computing expression 1:\n\n# init our tensors\na1 = torch.tensor([[2, 4], [5, 7]])\na2 = torch.tensor([[1, 1], [2, 3]])\na3 = torch.tensor([[10, 10], [12, 1]])\n## uncomment to test your function\nA = simple_operations(a1, a2, a3)\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[20, 24],\n        [31, 27]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef dot_product(b1: torch.Tensor, b2: torch.Tensor):\n  ###############################################\n  ## TODO for students:  complete the first computation using the argument matricies\n  \n  ###############################################\n  \"\"\"\n  Helper function to demonstrate dot product operation\n  Dot product is an algebraic operation that takes two equal-length sequences\n  (usually coordinate vectors), and returns a single number.\n  Geometrically, it is the product of the Euclidean magnitudes of the\n  two vectors and the cosine of the angle between them.\n\n  Args:\n    b1: Torch tensor\n      Tensor of size ([3])\n    b2: Torch tensor\n      Tensor of size ([3])\n\n  Returns:\n    product: Tensor\n      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n  \"\"\"\n  # Use torch.dot() to compute the dot product of two tensors\n  product = torch.dot(b1, b2)\n  return product\n\n# add timing to airtable\n\n\n# Computing expression 2:\nb1 = torch.tensor([3, 5, 7])\nb2 = torch.tensor([2, 4, 8])\n## Uncomment to test your function\nb = dot_product(b1, b2)\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor(82)\n```\n:::\n:::\n\n\nIrrelevant dimensions\n\nThere is a command `.squeeze` that gets rid of the singleton dimension it allow for indexing into the first dimension of a rank 1 tensor more simply.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nx = torch.randn(1,10)\nprint(x.shape)\nprint(\"x[0]: \", x[0])\n\nx = x.squeeze(0)\nprint(x.shape)\nprint(\"x[0]: \", x[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1, 10])\nx[0]:  tensor([-0.0612, -0.6197, -0.3718, -1.2872, -0.5185,  0.0805,  1.9483,  2.0819,\n        -0.9010,  1.1303])\ntorch.Size([10])\nx[0]:  tensor(-0.0612)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}