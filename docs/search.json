[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DsBlog",
    "section": "",
    "text": "Chemistry\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Week 1 Working Notes",
    "section": "",
    "text": "- Lec: Conceptual introduction to computational chemistry\n- Lab: IQMOL, exploring potential energy surfaces with IQMOL\nComputational chemistry is a field in which computational approaches are used to tackle chemical problems. These computational approaches allow us to rationalise experimental observations, predict reaction outcomes and design improved chemical reagents, catalysts and materials. The most powerful computational approaches involve supercomputers which allow the field to dive into areas of study such as quantum chemistry, molecular mechanics and also use machine learning to approach these same chemical problems.\nComputational chemistry can be used to perform calculations such as:\n- Atomic and molecular structure and energies (electronic, vibrational, rotational) of ground and excited states (spectra).\n- Thermodynamic properties and rate coefficients and equilibrium constants.\n- Dynamics (radial distribution functions and lifetimes)\n- Molecular properties such as molecular orbitals, atomic charges and electrostatic potential surfaces.\nA potential energy surface describes how the potential energy of a molecular changes with nuclear coordinates.\nCan derive a description of molecular orbitals by solving the time-independent S.E which requires the use of basis functions (basis sets).\nA basis set is a set of functions (basis functions) that are used to represent the electronic wave function within the Hartree-Fock method or density-functional theory, this allows the transformation of the partial differential equations within the model into algebraic equations that are more easily solvable via computational methods. Orbitals are expanded within the basis set as a linear combination of the basis functions. Most commonly the basis set is composed of atomic orbitals and this is referred to as the linear combination of atomic orbitals approach.\nSeveral types of atomic orbitals are often used: Gaussian-type orbitals, Slater-type orbitals or numerical atomic orbitals. Out of these options, Gaussian-type orbitals are most commonly used as they are less computationally demanding.\nWithin the basis set, the wavefunction is represented using a vector, the components of this vector correspond to the coefficients of the basis functions within the linear expansion.\nWhen performing molecular calculations, the common approach is to use a basis composed to atomic orbitals that are centered at each nucleus within a molecule.\nThe most physically reasonable basis set is the Slater-type orbitals, these are solutions to the Schrodinger equation of hydrogen like atoms.\n- Decay exponentially far away from the nucleus.\n- Satisfies Kato’s cusp condition at the nucleus, meaning that they are able to accurately describe electron density near the nucleus.\nAn issue that arises from the use of hydrogen like atoms is that these atoms lack ‘many-electron’ interactions and correspondingly their orbitals also lack the ability to represent these interactions (electron state correlations).\nWhilst Slater-type orbitals are the most physically reasonable basis set, calculating integrals with this set is very computationally challenging and thus a new approach was required. Using linear combinations of Gaussian-type orbitals to approximate a Slater-type orbital leads to substantial computational savings. The savings associated with this alternative process arise from the fact that when using the linear combination of Gaussian-type orbitals, integrals with Gaussian basis functions can be written in a closed form. (what does this mean?).\nThere exists a large library of basis sets, these sets typically are arranged in a hierarchy of increasing size. A larger basis set allows for the derivation of more accurate solutions however this comes with a increasingly large computational cost.\nMinimal basis sets are the smallest of the basis sets. Each atom in a molecule has a single basis function used for each orbital in a Hartree-Fock calculation on the free atom.\nPolarisation functions: additional functions that are added to the set to describe the polarisation of electron density of an atom within molecules. This addition allows for more flexibility of the basis set, which in turn allows the molecular orbitals to be more asymmetric about the nucleus. These functions are important for chemical bonding because bonds are often polarised.\nDiffuse functions:\n- Extended Gaussian functions with a small exponent that gives flexibility to the tail portion of the atomic orbitals distal from the nucleus.\n- These diffuse basis functions are important when considering anions or dipole moments and additionally can be important for the accurate modeling of intra and inter molecular bonding.\nSplit-valence basis sets:\n- Most commonly during molecular bonding, it is the valence electrons that are principally participating in the bonding. Thus representing these valence orbitals with more than one basis function is a common approach. The nomenclature for these split basis sets are double, triple, quadruple zeta etc basis sets. As the varying orbitals within the split basis sets have different spatial extents, it provides the set with the flexibility to adjust its electron density appropriately for a given molecular environment. This is an improvement relative to minimal basis sets who lack the flexibility to adjust to these particular molecular environments."
  },
  {
    "objectID": "posts/Week-1/index.html",
    "href": "posts/Week-1/index.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Note: Information in this article is derived from the book ‘Practical Statistics for Data Scientists’.\nSimple linear regression can be used to derive a model of the relationship present between the magnitude of one variable and that of another.\nThe dataset below contains records of the numbers of years a given worker has been exposed to cotton dust (‘Exposure’) versus a metric used to describe ones lung capacity (peak expiratory flow rate, ‘PEFR’).\nFrom this graph it is hard to judge the relationship between these two variables by eye, especially if one was intending to make a prediction from the association of the two variables.\n\nlung = pd.read_csv('LungDisease.csv')\nsns.scatterplot(data=lung, x='Exposure',y='PEFR')\n\n<AxesSubplot:xlabel='Exposure', ylabel='PEFR'>\n\n\n\n\n\nIf one did intend to make a prediction regarding the lung capacity of a worker after some defined period of exposure, a linear regression model would be appropriate.\nSimple linear regression provides an estimate of how much the response variable, Y will change when the predictor variable, X changes by a specified amount. This regression relationship is given by:\n\\(Y=b_0+b_1X\\)\nWhere;\n\\(b_0\\) = Intercept\n\\(b_1\\) = Slope\n\npredictors = ['Exposure']\noutcome = 'PEFR'\nlm = LinearRegression()\nlm.fit(lung[predictors], lung[outcome])\nprint(f'Intercept: {lm.intercept_:.3f}')\nprint(f'Coefficient Exposure: {lm.coef_[0]:.3f}')\n\nIntercept: 424.583\nCoefficient Exposure: -4.185\n\n\nThe intercept can be interpreted in this context as the predicted lung capacity or ‘PEFR’ of a worker who has had zero years of exposure to cotton dust. The regression coefficient or slope can be interpreted as the change in lung capacity for each year of exposure to cotton dust.\nSo it can be seen that a worker who has yet to be exposed to cotton dust is predicted to have a PEFR value of 424.583 and for every year of exposure to cotton dust that value is expected to decrease by 4.185.\n\nsns.regplot(data=lung,x='Exposure', y='PEFR')\n\n<AxesSubplot:xlabel='Exposure', ylabel='PEFR'>\n\n\n\n\n\nThe regression line is the plot above indicates the ‘fitted values’ or the predictions derived from the linear regression model. It can be seen that generally the data doesn’t fall exactly on the regression line and this deviation can be quantified as the ‘residuals’.\nThis concept of residual values underlines the manner in which our linear model is fit to the data set. The regression line itself is defined as the estimate that minimises the sum of the squared residual values, this is known as the residual sum of squares (RSS). This method of minimising the RSS is also referred to as least squares regression or ordinary least squares regression.\nRegression analysis is primarily used to bring light to a supposed linear relationship between a set of predictor variables and a response variable. Our primary goal is to understand this association and explain it using the data one has. In the case of this dataset is can be seen that exposure to cotton dust has a deleterious effect on the lung capacity of workers and predictions of this effect can be quantified using the derived regression coefficient (\\(\\hat{b}\\)).\nReferences:\nBruce, Peter, Andrew Bruce, and Peter Gedeck. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media, 2020."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html#terms",
    "href": "posts/post-with-code/index.html#terms",
    "title": "Week 1 Working Notes",
    "section": "Terms",
    "text": "Terms\n- Radial distribution function: Within a system of particles (atoms, molecules) describes how density varies as a function of distance from a reference particle. [Reference](https://chem.libretexts.org/Bookshelves/Biological_Chemistry/Concepts_in_Biophysical_Chemistry_(Tokmakoff)/01%3A_Water_and_Aqueous_Solutions/01%3A_Fluids/1.02%3A_Radial_Distribution_Function)\n- Electron correlation: the interactions between electrons within the electronic structure of a quantum system. The correlation energy is a measure of how much the movement of a given electron is influenced by the presence of all other electrons within the system."
  },
  {
    "objectID": "posts/post-with-code/index.html#references",
    "href": "posts/post-with-code/index.html#references",
    "title": "Week 1 Working Notes",
    "section": "References",
    "text": "References\n1. https://en.wikipedia.org/wiki/Basis_set_(chemistry)"
  },
  {
    "objectID": "posts/Week 1 Working Notes/index.html",
    "href": "posts/Week 1 Working Notes/index.html",
    "title": "Week 1 Working Notes",
    "section": "",
    "text": "- Lec: Conceptual introduction to computational chemistry\n- Lab: IQMOL, exploring potential energy surfaces with IQMOL\nComputational chemistry is a field in which computational approaches are used to tackle chemical problems. These computational approaches allow us to rationalise experimental observations, predict reaction outcomes and design improved chemical reagents, catalysts and materials. The most powerful computational approaches involve supercomputers which allow the field to dive into areas of study such as quantum chemistry, molecular mechanics and also use machine learning to approach these same chemical problems.\nComputational chemistry can be used to perform calculations such as:\n- Atomic and molecular structure and energies (electronic, vibrational, rotational) of ground and excited states (spectra).\n- Thermodynamic properties and rate coefficients and equilibrium constants.\n- Dynamics (radial distribution functions and lifetimes)\n- Molecular properties such as molecular orbitals, atomic charges and electrostatic potential surfaces.\nA potential energy surface describes how the potential energy of a molecular changes with nuclear coordinates.\nCan derive a description of molecular orbitals by solving the time-independent S.E which requires the use of basis functions (basis sets).\nA basis set is a set of functions (basis functions) that are used to represent the electronic wave function within the Hartree-Fock method or density-functional theory, this allows the transformation of the partial differential equations within the model into algebraic equations that are more easily solvable via computational methods. Orbitals are expanded within the basis set as a linear combination of the basis functions. Most commonly the basis set is composed of atomic orbitals and this is referred to as the linear combination of atomic orbitals approach.\nSeveral types of atomic orbitals are often used: Gaussian-type orbitals, Slater-type orbitals or numerical atomic orbitals. Out of these options, Gaussian-type orbitals are most commonly used as they are less computationally demanding.\nWithin the basis set, the wavefunction is represented using a vector, the components of this vector correspond to the coefficients of the basis functions within the linear expansion.\nWhen performing molecular calculations, the common approach is to use a basis composed to atomic orbitals that are centered at each nucleus within a molecule.\nThe most physically reasonable basis set is the Slater-type orbitals, these are solutions to the Schrodinger equation of hydrogen like atoms.\n- Decay exponentially far away from the nucleus.\n- Satisfies Kato’s cusp condition at the nucleus, meaning that they are able to accurately describe electron density near the nucleus.\nAn issue that arises from the use of hydrogen like atoms is that these atoms lack ‘many-electron’ interactions and correspondingly their orbitals also lack the ability to represent these interactions (electron state correlations).\nWhilst Slater-type orbitals are the most physically reasonable basis set, calculating integrals with this set is very computationally challenging and thus a new approach was required. Using linear combinations of Gaussian-type orbitals to approximate a Slater-type orbital leads to substantial computational savings. The savings associated with this alternative process arise from the fact that when using the linear combination of Gaussian-type orbitals, integrals with Gaussian basis functions can be written in a closed form. (what does this mean?).\nThere exists a large library of basis sets, these sets typically are arranged in a hierarchy of increasing size. A larger basis set allows for the derivation of more accurate solutions however this comes with a increasingly large computational cost.\nMinimal basis sets are the smallest of the basis sets. Each atom in a molecule has a single basis function used for each orbital in a Hartree-Fock calculation on the free atom.\nPolarisation functions: additional functions that are added to the set to describe the polarisation of electron density of an atom within molecules. This addition allows for more flexibility of the basis set, which in turn allows the molecular orbitals to be more asymmetric about the nucleus. These functions are important for chemical bonding because bonds are often polarised.\nDiffuse functions:\n- Extended Gaussian functions with a small exponent that gives flexibility to the tail portion of the atomic orbitals distal from the nucleus.\n- These diffuse basis functions are important when considering anions or dipole moments and additionally can be important for the accurate modeling of intra and inter molecular bonding.\nSplit-valence basis sets:\n- Most commonly during molecular bonding, it is the valence electrons that are principally participating in the bonding. Thus representing these valence orbitals with more than one basis function is a common approach. The nomenclature for these split basis sets are double, triple, quadruple zeta etc basis sets. As the varying orbitals within the split basis sets have different spatial extents, it provides the set with the flexibility to adjust its electron density appropriately for a given molecular environment. This is an improvement relative to minimal basis sets who lack the flexibility to adjust to these particular molecular environments."
  },
  {
    "objectID": "posts/Week 1 Working Notes/index.html#terms",
    "href": "posts/Week 1 Working Notes/index.html#terms",
    "title": "Week 1 Working Notes",
    "section": "Terms",
    "text": "Terms\n- Radial distribution function: Within a system of particles (atoms, molecules) describes how density varies as a function of distance from a reference particle. [Reference](https://chem.libretexts.org/Bookshelves/Biological_Chemistry/Concepts_in_Biophysical_Chemistry_(Tokmakoff)/01%3A_Water_and_Aqueous_Solutions/01%3A_Fluids/1.02%3A_Radial_Distribution_Function)\n- Electron correlation: the interactions between electrons within the electronic structure of a quantum system. The correlation energy is a measure of how much the movement of a given electron is influenced by the presence of all other electrons within the system."
  },
  {
    "objectID": "posts/Week 1 Working Notes/index.html#references",
    "href": "posts/Week 1 Working Notes/index.html#references",
    "title": "Week 1 Working Notes",
    "section": "References",
    "text": "References\n1. https://en.wikipedia.org/wiki/Basis_set_(chemistry)"
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html",
    "title": "Computational Methods for Drug Repositioning",
    "section": "",
    "text": "This paper outlines the use of computational chemistry methods to identify approved drugs that display an inhibitory effect against the main protease of SARS-CoV-2. This process is known as drug repurposing but is also termed as drug repositioning or therapeutic switching. This approach is used to identify novel therapeutic agents from a pool of existing FDA approved drug molecules. This approach is highly relevant as the drug discovery process is expensive, time-consuming, and financially risky. Thus, drug repositioning is employed to increase the success rate of drug development by offering numerous advantages relative to tradition drug discovery processes. These include a substantial reduction in the duration of the drug development process, lower-cost and higher efficiency and a significantly lower risk of failure[1]. The COVID-19 global pandemic highlighted an urgent need to develop effective therapeutic agents in a much shorter timeframe than traditional drug discovery processes allow for, thus drug repositioning efforts have been explored thoroughly as part of the effort to identify drug molecules for the treatment of COVID-19[1]. One of the predominant strategies used in drug-repurposing approaches is the use of computational methods such as molecular docking or molecular similarity. Computational methods follow along with the benefits of drug-repurposing approaches in that they allow for a time and cost efficient approach to identifying bioactive molecules to be examined for potential use as pharmacological agents[2].\nSARS-COV-2 has two proteases: chymotrypsin-like cysteine and main protease[3]. These enzymes are responsible for processing polyproteins which have been translated from viral RNA[4]. Inhibition of these proteases would result in an impediment of viral replication thereby resulting in a therapeutic effect[4]. It is also of note that no human proteases are known to share a similar cleavage specificity thus this inhibitory effect is unlikely to induce a toxic effect[4].\nThe rise of Omicron subvariants that carry mutations in their spike proteins leads to concerns regarding the efficacy of neutralising antibodies and correspondingly the efficacy of existing vaccines and therapeutic treatments[5]. Thus, the need for novel and efficacious chemotherapeutic treatments is clearly apparent. Drug repurposing efforts powered by computational chemistry methods (virtual screening/docking studies) provides a systematic path for ongoing drug discovery efforts to develop therapies effective against SARS-COV-2."
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html#design",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html#design",
    "title": "Computational Methods for Drug Repositioning",
    "section": "Design",
    "text": "Design\nThe authors began by applying a consensus molecular docking protocol to scan a virtual library of approved drugs ($$2000 drugs). A high-resolution crystal structure of main protease (Figure 1) that was co-crystallized with a non-covalent small fragment hit was used as the basis ligand of this docking study[3]. This decision was made as the presence of a ligand within this crystal structure would most likely alter the conformation of the active site side chain residues in a manner that would be better for the performance of molecular docking. The molecular docking protocol of this study included four independent trials of protein-ligand docking. The docking programs Glide SP, AutoDock Vina and AutoDock 4.2 were utilised, with AutoDock 4.2 being used for two of the protocols[3]. Following this, a ranking of the compounds in order of their docking scores was conducted and the top 200 hits from each docking trial were compared. From these hits compounds that were in the top 200 in all four or at least three of the trials were selected to proceed to the next stage of the experiment. This docking protocol narrowed the library down to 42 candidates which were then extensively analyzed with respect to conformation, stability in molecular dynamics simulations and consideration of intermolecular contacts (cite). This further narrowed down the candidates to 17, which were then examined via kinetic assays for inhibition of mPRO."
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html#computational-methods",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html#computational-methods",
    "title": "Computational Methods for Drug Repositioning",
    "section": "Computational Methods",
    "text": "Computational Methods\nVirtual screening is a computational approach for predicting potentially bioactive compounds when scanning a library of small molecules [2]. The authors of this paper utilised this approach in an attempt to discover small molecule inhibitors of main protease (mPRO)[3] . They utilised a form of virtual screening that is receptor based, specifically they used protein-ligand docking in order to identify drug molecules that had a potential inhibitory effect on mPRO[3].\n\nThis technique utilises a crystallised structure of a protein in order to predict how the compounds within the virtual library would bind to the binding site of a protein[2]. With this, they analysed more than 50 crystal structures of main protease in both apo and holo forms to select a structure that would perform optimally in a molecular docking study. Ultimately selecting a holo crystallised structure of the SARS-CoV-2 main protease as it was determined that the presence of a ligand bound to the crystal structure would position the active site residues in such a manner that would be conducive to optimal performance of the molecular docking study[3].\nAs part of this technique, the protein used for the basis of docking must be prepared. As this is an experimentally obtained structure, there will be aberrations that must be corrected to ensure optimal performance of the docking protocol[2]. X-ray crystallographic protein structures have common issues such as missing hydrogen atoms, missing residues, incomplete sidechains, undefined protonation states and/or presence of crystallization products that are not found in reality[2]. In order to mitigate these issues the authors of this paper utilised the program, Reduce, on the crystal structure in order to perform alterations such as side-chain flips, optimisation of hydrogen bonds and the addition or removal of hydrogen bonds[3]. To validate that this structure was chemically sound, a further program, UCSF Chimera, was used to visually analyse the structure[3, 6]. The next stage of the virtual screening protocol was docking. Generally, docking programs generate an initial set of conformational, tautomeric and protonation states for each of the ligand compounds[3]. Then, researchers can apply a search algorithm and scoring function to generate and score the poses of the ligand within the binding site of the receptor in an effort to identify the docked pose that likely represents the real binding mode of the compound[2-3]. Researchers have the autonomy to introduce constraints to these processes to assist these protocols in producing results that make chemical sense. For instance, the cavity of the protein in which compounds can be docked is commonly constrained in order to limit the space that can be occupied by docked poses[2]. Additionally, constraints regarding the specific binding modes of the docked poses can be implemented to limit the conformational space the search algorithm can select docked poses from[2].\nThe authors of this paper ran four independent docking protocols within this study using Glide SP, AutoDock Vina and two protocols utilising AutoDock 4.2[3]. The Glide software package utilises a set of hierarchical filters to search for probable locations of the ligand within the binding site region of the receptor[7]. The receptors shape and properties are represented using a grid via the use of varying sets of fields that yield a progressively more accurate scoring of the ligand pose[7]. Here, the creators of the Glide software package define the term ‘pose’ to refer to a complete specification of the ligand in terms of its position and orientation in space relative to the receptor, including its core and rotamer group conformations[7]. Following the field generation, generation of initial ligand conformations is performed. This set of conformations are selected from a listing of the minima energy structures within the ligand dihedral angle space[7]. Utilising these generated conformations, an initial screening procedure is performed to identify promising poses, this pre-screening step reduces the region where more computationally expensive evaluations are performed thereby allowing for acceptable computational speed[7]. Beginning with the promising poses identified from this initial screening, the ligands are the minimised within the field of the receptor via the use of a standard molecular mechanics energy function in order to select for a smaller sub-selection of low energy poses[7]. To finally predict the binding affinity and order the screened ligands, a combination of ‘GlideScore’, the ligand-receptor molecular mechanics energy and the ligand strain energy is utilised in order to identify correctly docked poses[7]."
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html#references",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html#references",
    "title": "Computational Methods for Drug Repositioning",
    "section": "References",
    "text": "References\n\nB. M. Sahoo, B. V. V. Ravi Kumar, J. Sruti, M. K. Mahapatra, B. K. Banik and P. Borah, Frontiers in Molecular Biosciences 2021, 8.\n\n\n\nA. Gimeno, M. Ojeda-Montes, S. Tomás-Hernández, A. Cereto-Massagué, R. Beltrán-Debón, M. Mulero, G. Pujadas and S. Garcia-Vallvé, International Journal of Molecular Sciences 2019, 20, 1375.\nM. M. Ghahremanpour, J. Tirado-Rives, M. Deshmukh, J. A. Ippolito, C.-H. Zhang, I. Cabeza De Vaca, M.-E. Liosi, K. S. Anderson and W. L. Jorgensen, ACS Medicinal Chemistry Letters 2020, 11, 2526-2533.\nZ. Jin, X. Du, Y. Xu, Y. Deng, M. Liu, Y. Zhao, B. Zhang, X. Li, L. Zhang, C. Peng, Y. Duan, J. Yu, L. Wang, K. Yang, F. Liu, R. Jiang, X. Yang, T. You, X. Liu, X. Yang, F. Bai, H. Liu, X. Liu, L. W. Guddat, W. Xu, G. Xiao, C. Qin, Z. Shi, H. Jiang, Z. Rao and H. Yang, Nature 2020, 582, 289-293.\nQ. Wang, Y. Guo, S. Iketani, M. S. Nair, Z. Li, H. Mohri, M. Wang, J. Yu, A. D. Bowen, J. Y. Chang, J. G. Shah, N. Nguyen, Z. Chen, K. Meyers, M. T. Yin, M. E. Sobieszczyk, Z. Sheng, Y. Huang, L. Liu and D. D. Ho, Nature 2022, 608, 603-608.\nE. F. Pettersen, T. D. Goddard, C. C. Huang, G. S. Couch, D. M. Greenblatt, E. C. Meng and T. E. Ferrin, Journal of Computational Chemistry 2004, 25, 1605-1612.\nR. A. Friesner, J. L. Banks, R. B. Murphy, T. A. Halgren, J. J. Klicic, D. T. Mainz, M. P. Repasky, E. H. Knoll, M. Shelley, J. K. Perry, D. E. Shaw, P. Francis and P. S. Shenkin, Journal of Medicinal Chemistry 2004, 47, 1739-1749."
  },
  {
    "objectID": "tidyr.html",
    "href": "tidyr.html",
    "title": "dplyr and tidyr",
    "section": "",
    "text": "Ways in which we can access information\n\nExtracting existing variables: select()\nExtract existing observations: filter()\nDrive new variables from existing ones: mutate()\nChange the unit of analysis: summarise()\n\nselect()\nThis function allows for the subsection of a smaller number of columns from a larger dataframe.\n\nlibrary(EDAWR)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:EDAWR':\n\n    storms\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n\nAttaching package: 'tidyr'\n\n\nThe following objects are masked from 'package:EDAWR':\n\n    population, who\n\nlibrary(knitr)\ndf = storms\nsubset = select(df, name, pressure)\nkable((head(subset)))\n\n\n\n\nname\npressure\n\n\n\n\nAmy\n1013\n\n\nAmy\n1013\n\n\nAmy\n1013\n\n\nAmy\n1013\n\n\nAmy\n1012\n\n\nAmy\n1012\n\n\n\n\n\nDropping a column.\n\nselect(storms, -name)\n\n# A tibble: 11,859 × 12\n    year month   day  hour   lat  long status      categ…¹  wind press…² tropi…³\n   <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>       <ord>   <int>   <int>   <int>\n 1  1975     6    27     0  27.5 -79   tropical d… -1         25    1013      NA\n 2  1975     6    27     6  28.5 -79   tropical d… -1         25    1013      NA\n 3  1975     6    27    12  29.5 -79   tropical d… -1         25    1013      NA\n 4  1975     6    27    18  30.5 -79   tropical d… -1         25    1013      NA\n 5  1975     6    28     0  31.5 -78.8 tropical d… -1         25    1012      NA\n 6  1975     6    28     6  32.4 -78.7 tropical d… -1         25    1012      NA\n 7  1975     6    28    12  33.3 -78   tropical d… -1         25    1011      NA\n 8  1975     6    28    18  34   -77   tropical d… -1         30    1006      NA\n 9  1975     6    29     0  34.4 -75.8 tropical s… 0          35    1004      NA\n10  1975     6    29     6  34   -74.8 tropical s… 0          40    1002      NA\n# … with 11,849 more rows, 1 more variable: hurricane_force_diameter <int>, and\n#   abbreviated variable names ¹​category, ²​pressure,\n#   ³​tropicalstorm_force_diameter\n\n\nObtaining a range of variables.\n\nselect(storms, wind:year)\n\n# A tibble: 11,859 × 9\n    wind category status               long   lat  hour   day month  year\n   <int> <ord>    <chr>               <dbl> <dbl> <dbl> <int> <dbl> <dbl>\n 1    25 -1       tropical depression -79    27.5     0    27     6  1975\n 2    25 -1       tropical depression -79    28.5     6    27     6  1975\n 3    25 -1       tropical depression -79    29.5    12    27     6  1975\n 4    25 -1       tropical depression -79    30.5    18    27     6  1975\n 5    25 -1       tropical depression -78.8  31.5     0    28     6  1975\n 6    25 -1       tropical depression -78.7  32.4     6    28     6  1975\n 7    25 -1       tropical depression -78    33.3    12    28     6  1975\n 8    30 -1       tropical depression -77    34      18    28     6  1975\n 9    35 0        tropical storm      -75.8  34.4     0    29     6  1975\n10    40 0        tropical storm      -74.8  34       6    29     6  1975\n# … with 11,849 more rows\n\n\nfilter()\nFilter out observations using logical tests.\n\nfilter(storms, wind >= 50, name %in% c('Alberto', 'Alex', 'Allison'))\n\n# A tibble: 128 × 13\n   name     year month   day  hour   lat  long status      categ…¹  wind press…²\n   <chr>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>       <ord>   <int>   <int>\n 1 Alberto  1982     6     3    12  23.2 -84.2 tropical s… 0          50     995\n 2 Alberto  1982     6     3    18  24   -83.6 hurricane   1          75     985\n 3 Alberto  1982     6     4     0  24.8 -83.4 hurricane   1          65     992\n 4 Alberto  1982     6     4     6  24.9 -84.1 tropical s… 0          55     998\n 5 Alberto  1994     7     3     6  28.8 -86.8 tropical s… 0          50     997\n 6 Alberto  1994     7     3    12  29.9 -86.7 tropical s… 0          55     993\n 7 Alberto  1994     7     3    15  30.4 -86.5 tropical s… 0          55     993\n 8 Allison  1995     6     4     0  22   -86   tropical s… 0          50     997\n 9 Allison  1995     6     4     6  23.3 -86.3 tropical s… 0          60     995\n10 Allison  1995     6     4    12  24.7 -86.2 hurricane   1          65     987\n# … with 118 more rows, 2 more variables: tropicalstorm_force_diameter <int>,\n#   hurricane_force_diameter <int>, and abbreviated variable names ¹​category,\n#   ²​pressure\n\n\nmutate()\nMutate() allows you to generate new variables from existing ones.\n\ndf = mutate(storms, ratio = pressure/wind)\nselect(df, name, ratio)\n\n# A tibble: 11,859 × 2\n   name  ratio\n   <chr> <dbl>\n 1 Amy    40.5\n 2 Amy    40.5\n 3 Amy    40.5\n 4 Amy    40.5\n 5 Amy    40.5\n 6 Amy    40.5\n 7 Amy    40.4\n 8 Amy    33.5\n 9 Amy    28.7\n10 Amy    25.0\n# … with 11,849 more rows\n\n\nsummarise()\nCalculates summary statistics from a dataframe.\n\npollution %>% summarise(median = median(amount), variance = var(amount))\n\n  median variance\n1   22.5   1731.6\n\n\narrange()\nArranges the order of the rows within a dataframe based on the values within a column.\n\narrange(storms, wind)\n\n# A tibble: 11,859 × 13\n   name       year month   day  hour   lat  long status    categ…¹  wind press…²\n   <chr>     <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>     <ord>   <int>   <int>\n 1 Bonnie     1986     6    28     6  36.5 -91.3 tropical… -1         10    1013\n 2 Bonnie     1986     6    28    12  37.2 -90   tropical… -1         10    1012\n 3 AL031987   1987     8    16    18  30.9 -83.2 tropical… -1         10    1014\n 4 AL031987   1987     8    17     0  31.4 -82.9 tropical… -1         10    1015\n 5 AL031987   1987     8    17     6  31.8 -82.3 tropical… -1         10    1015\n 6 Alberto    1994     7     7     0  32.7 -86.3 tropical… -1         10    1012\n 7 Alberto    1994     7     7     6  32.7 -86.6 tropical… -1         10    1012\n 8 Alberto    1994     7     7    12  32.8 -86.8 tropical… -1         10    1012\n 9 Alberto    1994     7     7    18  33   -87   tropical… -1         10    1013\n10 Claudette  1979     7    27    12  34   -95.9 tropical… -1         15    1007\n# … with 11,849 more rows, 2 more variables:\n#   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n#   abbreviated variable names ¹​category, ²​pressure\n\narrange(storms, desc(wind))\n\n# A tibble: 11,859 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   <chr>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>     <ord>    <int>    <int>\n 1 Gilbert  1988     9    14     0  19.7 -83.8 hurricane 5          160      888\n 2 Wilma    2005    10    19    12  17.3 -82.8 hurricane 5          160      882\n 3 Dorian   2019     9     1    16  26.5 -77   hurricane 5          160      910\n 4 Dorian   2019     9     1    18  26.5 -77.1 hurricane 5          160      910\n 5 Gilbert  1988     9    14     6  19.9 -85.3 hurricane 5          155      889\n 6 Mitch    1998    10    26    18  16.9 -83.1 hurricane 5          155      905\n 7 Mitch    1998    10    27     0  17.2 -83.8 hurricane 5          155      910\n 8 Rita     2005     9    22     3  24.7 -87.3 hurricane 5          155      895\n 9 Rita     2005     9    22     6  24.8 -87.6 hurricane 5          155      897\n10 Dorian   2019     9     1    12  26.5 -76.5 hurricane 5          155      927\n# … with 11,849 more rows, and 2 more variables:\n#   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>\n\n\nAdding additional variables into the arrange function will use those as tiebreakers between observations that have the same value.\n\narrange(storms, wind,year)\n\n# A tibble: 11,859 × 13\n   name       year month   day  hour   lat  long status    categ…¹  wind press…²\n   <chr>     <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>     <ord>   <int>   <int>\n 1 Bonnie     1986     6    28     6  36.5 -91.3 tropical… -1         10    1013\n 2 Bonnie     1986     6    28    12  37.2 -90   tropical… -1         10    1012\n 3 AL031987   1987     8    16    18  30.9 -83.2 tropical… -1         10    1014\n 4 AL031987   1987     8    17     0  31.4 -82.9 tropical… -1         10    1015\n 5 AL031987   1987     8    17     6  31.8 -82.3 tropical… -1         10    1015\n 6 Alberto    1994     7     7     0  32.7 -86.3 tropical… -1         10    1012\n 7 Alberto    1994     7     7     6  32.7 -86.6 tropical… -1         10    1012\n 8 Alberto    1994     7     7    12  32.8 -86.8 tropical… -1         10    1012\n 9 Alberto    1994     7     7    18  33   -87   tropical… -1         10    1013\n10 Claudette  1979     7    27    12  34   -95.9 tropical… -1         15    1007\n# … with 11,849 more rows, 2 more variables:\n#   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n#   abbreviated variable names ¹​category, ²​pressure\n\n\nPipe Operator\n\nstorms %>%\n  filter(wind>=50) %>%\n  select(name, pressure)\n\n# A tibble: 5,756 × 2\n   name  pressure\n   <chr>    <int>\n 1 Amy        998\n 2 Amy        998\n 3 Amy        998\n 4 Amy        987\n 5 Amy        987\n 6 Amy        984\n 7 Amy        984\n 8 Amy        984\n 9 Amy        984\n10 Amy        984\n# … with 5,746 more rows\n\n\n\nstorms %>%\n  mutate(ratio = pressure/wind) %>%\n  select(name, ratio) \n\n# A tibble: 11,859 × 2\n   name  ratio\n   <chr> <dbl>\n 1 Amy    40.5\n 2 Amy    40.5\n 3 Amy    40.5\n 4 Amy    40.5\n 5 Amy    40.5\n 6 Amy    40.5\n 7 Amy    40.4\n 8 Amy    33.5\n 9 Amy    28.7\n10 Amy    25.0\n# … with 11,849 more rows\n\n\ngroup_by() + summarise()\n\ngrouped_pollution = pollution %>% group_by(city) %>% summarise(mean = mean(amount), sum = sum(amount), n = n())\nkable(grouped_pollution)\n\n\n\n\ncity\nmean\nsum\nn\n\n\n\n\nBeijing\n88.5\n177\n2\n\n\nLondon\n19.0\n38\n2\n\n\nNew York\n18.5\n37\n2\n\n\n\n\n\n\nlibrary(gtsummary)\ngrouped_pollution %>% select(city,mean,sum) %>% tbl_summary()\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 31\n    \n  \n  \n    city\n\n        Beijing\n1 (33%)\n        London\n1 (33%)\n        New York\n1 (33%)\n    mean\n\n        18.5\n1 (33%)\n        19\n1 (33%)\n        88.5\n1 (33%)\n    sum\n\n        37\n1 (33%)\n        38\n1 (33%)\n        177\n1 (33%)\n  \n  \n  \n    \n      1 n (%)"
  }
]