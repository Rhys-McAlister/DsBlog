[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DsBlog",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Week-1/index.html",
    "href": "posts/Week-1/index.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Note: Information in this article is derived from the book ‘Practical Statistics for Data Scientists’.\nSimple linear regression can be used to derive a model of the relationship present between the magnitude of one variable and that of another.\nThe dataset below contains records of the numbers of years a given worker has been exposed to cotton dust (‘Exposure’) versus a metric used to describe ones lung capacity (peak expiratory flow rate, ‘PEFR’).\nFrom this graph it is hard to judge the relationship between these two variables by eye, especially if one was intending to make a prediction from the association of the two variables.\n\nlung = pd.read_csv('LungDisease.csv')\nsns.scatterplot(data=lung, x='Exposure',y='PEFR')\n\n<AxesSubplot:xlabel='Exposure', ylabel='PEFR'>\n\n\n\n\n\nIf one did intend to make a prediction regarding the lung capacity of a worker after some defined period of exposure, a linear regression model would be appropriate.\nSimple linear regression provides an estimate of how much the response variable, Y will change when the predictor variable, X changes by a specified amount. This regression relationship is given by:\nY=b0+b1XY = b_0 + b_1 XY=b0​+b1​X\nWhere;\nb0b_0b0​: The intercept b1b_1b1​: Slope\n\npredictors = ['Exposure']\noutcome = 'PEFR'\nlm = LinearRegression()\nlm.fit(lung[predictors], lung[outcome])\nprint(f'Intercept: {lm.intercept_:.3f}')\nprint(f'Coefficient Exposure: {lm.coef_[0]:.3f}')\n\nIntercept: 424.583\nCoefficient Exposure: -4.185\n\n\nThe intercept can be interpreted in this context as the predicted lung capacity or ‘PEFR’ of a worker who has had zero years of exposure to cotton dust. The regression coefficient or slope can be interpreted as the change in lung capacity for each year of exposure to cotton dust.\nSo it can be seen that a worker who has yet to be exposed to cotton dust is predicted to have a PEFR value of 424.583 and for every year of exposure to cotton dust that value is expected to decrease by 4.185.\n\nsns.regplot(data=lung,x='Exposure', y='PEFR')\n\n<AxesSubplot:xlabel='Exposure', ylabel='PEFR'>\n\n\n\n\n\nThe regression line is the plot above indicates the ‘fitted values’ or the predictions derived from the linear regression model. It can be seen that generally the data doesn’t fall exactly on the regression line and this deviation can be quantified as the ‘residuals’.\nThis concept of residual values underlines the manner in which our linear model is fit to the data set. The regression line itself is defined as the estimate that minimises the sum of the squared residual values, this is known as the residual sum of squares (RSS). This method of minimising the RSS is also referred to as least squares regression or ordinary least squares regression.\nRegression analysis is primarily used to bring light to a supposed linear relationship between a set of predictor variables and a response variable. Our primary goal is to understand this association and explain it using the data one has. In the case of this dataset is can be seen that exposure to cotton dust has a deleterious effect on the lung capacity of workers and predictions of this effect can be quantified using the derived regression coefficient (b^b^).\nReferences:\nBruce, Peter, Andrew Bruce, and Peter Gedeck. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media, 2020."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]