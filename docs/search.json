[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DsBlog",
    "section": "",
    "text": "abstract\n\n\n\n\n\n\nOct 29, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\nA summary of a series of lectures on nanoparticulate drug delivery systems for cancer therapy.\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA record of my progress learning the Tidymodels library.\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nRhys McAlister\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Week 1 Working Notes",
    "section": "",
    "text": "- Lec: Conceptual introduction to computational chemistry\n- Lab: IQMOL, exploring potential energy surfaces with IQMOL\nComputational chemistry is a field in which computational approaches are used to tackle chemical problems. These computational approaches allow us to rationalise experimental observations, predict reaction outcomes and design improved chemical reagents, catalysts and materials. The most powerful computational approaches involve supercomputers which allow the field to dive into areas of study such as quantum chemistry, molecular mechanics and also use machine learning to approach these same chemical problems.\nComputational chemistry can be used to perform calculations such as:\n- Atomic and molecular structure and energies (electronic, vibrational, rotational) of ground and excited states (spectra).\n- Thermodynamic properties and rate coefficients and equilibrium constants.\n- Dynamics (radial distribution functions and lifetimes)\n- Molecular properties such as molecular orbitals, atomic charges and electrostatic potential surfaces.\nA potential energy surface describes how the potential energy of a molecular changes with nuclear coordinates.\nCan derive a description of molecular orbitals by solving the time-independent S.E which requires the use of basis functions (basis sets).\nA basis set is a set of functions (basis functions) that are used to represent the electronic wave function within the Hartree-Fock method or density-functional theory, this allows the transformation of the partial differential equations within the model into algebraic equations that are more easily solvable via computational methods. Orbitals are expanded within the basis set as a linear combination of the basis functions. Most commonly the basis set is composed of atomic orbitals and this is referred to as the linear combination of atomic orbitals approach.\nSeveral types of atomic orbitals are often used: Gaussian-type orbitals, Slater-type orbitals or numerical atomic orbitals. Out of these options, Gaussian-type orbitals are most commonly used as they are less computationally demanding.\nWithin the basis set, the wavefunction is represented using a vector, the components of this vector correspond to the coefficients of the basis functions within the linear expansion.\nWhen performing molecular calculations, the common approach is to use a basis composed to atomic orbitals that are centered at each nucleus within a molecule.\nThe most physically reasonable basis set is the Slater-type orbitals, these are solutions to the Schrodinger equation of hydrogen like atoms.\n- Decay exponentially far away from the nucleus.\n- Satisfies Kato’s cusp condition at the nucleus, meaning that they are able to accurately describe electron density near the nucleus.\nAn issue that arises from the use of hydrogen like atoms is that these atoms lack ‘many-electron’ interactions and correspondingly their orbitals also lack the ability to represent these interactions (electron state correlations).\nWhilst Slater-type orbitals are the most physically reasonable basis set, calculating integrals with this set is very computationally challenging and thus a new approach was required. Using linear combinations of Gaussian-type orbitals to approximate a Slater-type orbital leads to substantial computational savings. The savings associated with this alternative process arise from the fact that when using the linear combination of Gaussian-type orbitals, integrals with Gaussian basis functions can be written in a closed form. (what does this mean?).\nThere exists a large library of basis sets, these sets typically are arranged in a hierarchy of increasing size. A larger basis set allows for the derivation of more accurate solutions however this comes with a increasingly large computational cost.\nMinimal basis sets are the smallest of the basis sets. Each atom in a molecule has a single basis function used for each orbital in a Hartree-Fock calculation on the free atom.\nPolarisation functions: additional functions that are added to the set to describe the polarisation of electron density of an atom within molecules. This addition allows for more flexibility of the basis set, which in turn allows the molecular orbitals to be more asymmetric about the nucleus. These functions are important for chemical bonding because bonds are often polarised.\nDiffuse functions:\n- Extended Gaussian functions with a small exponent that gives flexibility to the tail portion of the atomic orbitals distal from the nucleus.\n- These diffuse basis functions are important when considering anions or dipole moments and additionally can be important for the accurate modeling of intra and inter molecular bonding.\nSplit-valence basis sets:\n- Most commonly during molecular bonding, it is the valence electrons that are principally participating in the bonding. Thus representing these valence orbitals with more than one basis function is a common approach. The nomenclature for these split basis sets are double, triple, quadruple zeta etc basis sets. As the varying orbitals within the split basis sets have different spatial extents, it provides the set with the flexibility to adjust its electron density appropriately for a given molecular environment. This is an improvement relative to minimal basis sets who lack the flexibility to adjust to these particular molecular environments."
  },
  {
    "objectID": "posts/Week-1/index.html",
    "href": "posts/Week-1/index.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Note: Information in this article is derived from the book ‘Practical Statistics for Data Scientists’.\nSimple linear regression can be used to derive a model of the relationship present between the magnitude of one variable and that of another.\nThe dataset below contains records of the numbers of years a given worker has been exposed to cotton dust (‘Exposure’) versus a metric used to describe ones lung capacity (peak expiratory flow rate, ‘PEFR’).\nFrom this graph it is hard to judge the relationship between these two variables by eye, especially if one was intending to make a prediction from the association of the two variables.\n\nlung = pd.read_csv('LungDisease.csv')\nsns.scatterplot(data=lung, x='Exposure',y='PEFR')\n\n<AxesSubplot:xlabel='Exposure', ylabel='PEFR'>\n\n\n\n\n\nIf one did intend to make a prediction regarding the lung capacity of a worker after some defined period of exposure, a linear regression model would be appropriate.\nSimple linear regression provides an estimate of how much the response variable, Y will change when the predictor variable, X changes by a specified amount. This regression relationship is given by:\n\\(Y=b_0+b_1X\\)\nWhere;\n\\(b_0\\) = Intercept\n\\(b_1\\) = Slope\n\npredictors = ['Exposure']\noutcome = 'PEFR'\nlm = LinearRegression()\nlm.fit(lung[predictors], lung[outcome])\nprint(f'Intercept: {lm.intercept_:.3f}')\nprint(f'Coefficient Exposure: {lm.coef_[0]:.3f}')\n\nIntercept: 424.583\nCoefficient Exposure: -4.185\n\n\nThe intercept can be interpreted in this context as the predicted lung capacity or ‘PEFR’ of a worker who has had zero years of exposure to cotton dust. The regression coefficient or slope can be interpreted as the change in lung capacity for each year of exposure to cotton dust.\nSo it can be seen that a worker who has yet to be exposed to cotton dust is predicted to have a PEFR value of 424.583 and for every year of exposure to cotton dust that value is expected to decrease by 4.185.\n\nsns.regplot(data=lung,x='Exposure', y='PEFR')\n\n<AxesSubplot:xlabel='Exposure', ylabel='PEFR'>\n\n\n\n\n\nThe regression line is the plot above indicates the ‘fitted values’ or the predictions derived from the linear regression model. It can be seen that generally the data doesn’t fall exactly on the regression line and this deviation can be quantified as the ‘residuals’.\nThis concept of residual values underlines the manner in which our linear model is fit to the data set. The regression line itself is defined as the estimate that minimises the sum of the squared residual values, this is known as the residual sum of squares (RSS). This method of minimising the RSS is also referred to as least squares regression or ordinary least squares regression.\nRegression analysis is primarily used to bring light to a supposed linear relationship between a set of predictor variables and a response variable. Our primary goal is to understand this association and explain it using the data one has. In the case of this dataset is can be seen that exposure to cotton dust has a deleterious effect on the lung capacity of workers and predictions of this effect can be quantified using the derived regression coefficient (\\(\\hat{b}\\)).\nReferences:\nBruce, Peter, Andrew Bruce, and Peter Gedeck. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media, 2020."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html#terms",
    "href": "posts/post-with-code/index.html#terms",
    "title": "Week 1 Working Notes",
    "section": "Terms",
    "text": "Terms\n- Radial distribution function: Within a system of particles (atoms, molecules) describes how density varies as a function of distance from a reference particle. [Reference](https://chem.libretexts.org/Bookshelves/Biological_Chemistry/Concepts_in_Biophysical_Chemistry_(Tokmakoff)/01%3A_Water_and_Aqueous_Solutions/01%3A_Fluids/1.02%3A_Radial_Distribution_Function)\n- Electron correlation: the interactions between electrons within the electronic structure of a quantum system. The correlation energy is a measure of how much the movement of a given electron is influenced by the presence of all other electrons within the system."
  },
  {
    "objectID": "posts/post-with-code/index.html#references",
    "href": "posts/post-with-code/index.html#references",
    "title": "Week 1 Working Notes",
    "section": "References",
    "text": "References\n1. https://en.wikipedia.org/wiki/Basis_set_(chemistry)"
  },
  {
    "objectID": "posts/Week 1 Working Notes/index.html",
    "href": "posts/Week 1 Working Notes/index.html",
    "title": "Week 1 Working Notes",
    "section": "",
    "text": "- Lec: Conceptual introduction to computational chemistry\n- Lab: IQMOL, exploring potential energy surfaces with IQMOL\nComputational chemistry is a field in which computational approaches are used to tackle chemical problems. These computational approaches allow us to rationalise experimental observations, predict reaction outcomes and design improved chemical reagents, catalysts and materials. The most powerful computational approaches involve supercomputers which allow the field to dive into areas of study such as quantum chemistry, molecular mechanics and also use machine learning to approach these same chemical problems.\nComputational chemistry can be used to perform calculations such as:\n- Atomic and molecular structure and energies (electronic, vibrational, rotational) of ground and excited states (spectra).\n- Thermodynamic properties and rate coefficients and equilibrium constants.\n- Dynamics (radial distribution functions and lifetimes)\n- Molecular properties such as molecular orbitals, atomic charges and electrostatic potential surfaces.\nA potential energy surface describes how the potential energy of a molecular changes with nuclear coordinates.\nCan derive a description of molecular orbitals by solving the time-independent S.E which requires the use of basis functions (basis sets).\nA basis set is a set of functions (basis functions) that are used to represent the electronic wave function within the Hartree-Fock method or density-functional theory, this allows the transformation of the partial differential equations within the model into algebraic equations that are more easily solvable via computational methods. Orbitals are expanded within the basis set as a linear combination of the basis functions. Most commonly the basis set is composed of atomic orbitals and this is referred to as the linear combination of atomic orbitals approach.\nSeveral types of atomic orbitals are often used: Gaussian-type orbitals, Slater-type orbitals or numerical atomic orbitals. Out of these options, Gaussian-type orbitals are most commonly used as they are less computationally demanding.\nWithin the basis set, the wavefunction is represented using a vector, the components of this vector correspond to the coefficients of the basis functions within the linear expansion.\nWhen performing molecular calculations, the common approach is to use a basis composed to atomic orbitals that are centered at each nucleus within a molecule.\nThe most physically reasonable basis set is the Slater-type orbitals, these are solutions to the Schrodinger equation of hydrogen like atoms.\n- Decay exponentially far away from the nucleus.\n- Satisfies Kato’s cusp condition at the nucleus, meaning that they are able to accurately describe electron density near the nucleus.\nAn issue that arises from the use of hydrogen like atoms is that these atoms lack ‘many-electron’ interactions and correspondingly their orbitals also lack the ability to represent these interactions (electron state correlations).\nWhilst Slater-type orbitals are the most physically reasonable basis set, calculating integrals with this set is very computationally challenging and thus a new approach was required. Using linear combinations of Gaussian-type orbitals to approximate a Slater-type orbital leads to substantial computational savings. The savings associated with this alternative process arise from the fact that when using the linear combination of Gaussian-type orbitals, integrals with Gaussian basis functions can be written in a closed form. (what does this mean?).\nThere exists a large library of basis sets, these sets typically are arranged in a hierarchy of increasing size. A larger basis set allows for the derivation of more accurate solutions however this comes with a increasingly large computational cost.\nMinimal basis sets are the smallest of the basis sets. Each atom in a molecule has a single basis function used for each orbital in a Hartree-Fock calculation on the free atom.\nPolarisation functions: additional functions that are added to the set to describe the polarisation of electron density of an atom within molecules. This addition allows for more flexibility of the basis set, which in turn allows the molecular orbitals to be more asymmetric about the nucleus. These functions are important for chemical bonding because bonds are often polarised.\nDiffuse functions:\n- Extended Gaussian functions with a small exponent that gives flexibility to the tail portion of the atomic orbitals distal from the nucleus.\n- These diffuse basis functions are important when considering anions or dipole moments and additionally can be important for the accurate modeling of intra and inter molecular bonding.\nSplit-valence basis sets:\n- Most commonly during molecular bonding, it is the valence electrons that are principally participating in the bonding. Thus representing these valence orbitals with more than one basis function is a common approach. The nomenclature for these split basis sets are double, triple, quadruple zeta etc basis sets. As the varying orbitals within the split basis sets have different spatial extents, it provides the set with the flexibility to adjust its electron density appropriately for a given molecular environment. This is an improvement relative to minimal basis sets who lack the flexibility to adjust to these particular molecular environments."
  },
  {
    "objectID": "posts/Week 1 Working Notes/index.html#terms",
    "href": "posts/Week 1 Working Notes/index.html#terms",
    "title": "Week 1 Working Notes",
    "section": "Terms",
    "text": "Terms\n- Radial distribution function: Within a system of particles (atoms, molecules) describes how density varies as a function of distance from a reference particle. [Reference](https://chem.libretexts.org/Bookshelves/Biological_Chemistry/Concepts_in_Biophysical_Chemistry_(Tokmakoff)/01%3A_Water_and_Aqueous_Solutions/01%3A_Fluids/1.02%3A_Radial_Distribution_Function)\n- Electron correlation: the interactions between electrons within the electronic structure of a quantum system. The correlation energy is a measure of how much the movement of a given electron is influenced by the presence of all other electrons within the system."
  },
  {
    "objectID": "posts/Week 1 Working Notes/index.html#references",
    "href": "posts/Week 1 Working Notes/index.html#references",
    "title": "Week 1 Working Notes",
    "section": "References",
    "text": "References\n1. https://en.wikipedia.org/wiki/Basis_set_(chemistry)"
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html",
    "title": "Computational Methods for Drug Repositioning",
    "section": "",
    "text": "This paper outlines the use of computational chemistry methods to identify approved drugs that display an inhibitory effect against the main protease of SARS-CoV-2. This process is known as drug repurposing but is also termed as drug repositioning or therapeutic switching. This approach is used to identify novel therapeutic agents from a pool of existing FDA approved drug molecules. This approach is highly relevant as the drug discovery process is expensive, time-consuming, and financially risky. Thus, drug repositioning is employed to increase the success rate of drug development by offering numerous advantages relative to tradition drug discovery processes. These include a substantial reduction in the duration of the drug development process, lower-cost and higher efficiency and a significantly lower risk of failure[1]. The COVID-19 global pandemic highlighted an urgent need to develop effective therapeutic agents in a much shorter timeframe than traditional drug discovery processes allow for, thus drug repositioning efforts have been explored thoroughly as part of the effort to identify drug molecules for the treatment of COVID-19[1]. One of the predominant strategies used in drug-repurposing approaches is the use of computational methods such as molecular docking or molecular similarity. Computational methods follow along with the benefits of drug-repurposing approaches in that they allow for a time and cost efficient approach to identifying bioactive molecules to be examined for potential use as pharmacological agents[2].\nSARS-COV-2 has two proteases: chymotrypsin-like cysteine and main protease[3]. These enzymes are responsible for processing polyproteins which have been translated from viral RNA[3]. Inhibition of these proteases would result in an impediment of viral replication thereby resulting in a therapeutic effect[3]. It is also of note that no human proteases are known to share a similar cleavage specificity thus this inhibitory effect is unlikely to induce a toxic effect[3].\nThe rise of Omicron subvariants that carry mutations in their spike proteins leads to concerns regarding the efficacy of neutralising antibodies and correspondingly the efficacy of existing vaccines and therapeutic treatments[4]. Thus, the need for novel and efficacious chemotherapeutic treatments is clearly apparent. Drug repurposing efforts powered by computational chemistry methods (virtual screening/docking studies) provides a systematic path for ongoing drug discovery efforts to develop therapies effective against SARS-COV-2."
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html#design",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html#design",
    "title": "Computational Methods for Drug Repositioning",
    "section": "Design",
    "text": "Design\nThe authors began by applying a consensus molecular docking protocol to scan a virtual library of approved drugs (around 2000 drugs). A high-resolution crystal structure of main protease (Figure 1) that was co-crystallized with a non-covalent small fragment hit was used as the basis ligand of this docking study[5]. This decision was made as the presence of a ligand within this crystal structure would most likely alter the conformation of the active site side chain residues in a manner that would be better for the performance of molecular docking. The molecular docking protocol of this study included four independent trials of protein-ligand docking. The docking programs Glide SP, AutoDock Vina and AutoDock 4.2 were utilised, with AutoDock 4.2 being used for two of the protocols[3]. Following this, a ranking of the compounds in order of their docking scores was conducted and the top 200 hits from each docking trial were compared. From these hits compounds that were in the top 200 in all four or at least three of the trials were selected to proceed to the next stage of the experiment. This docking protocol narrowed the library down to 42 candidates which were then extensively analyzed with respect to conformation, stability in molecular dynamics simulations and consideration of intermolecular contacts. This further narrowed down the candidates to 17, which were then examined via kinetic assays for inhibition of mPRO.\n\n\n\nRendering of the catalytic site of the crystal structure of Main Protease [3]"
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html#computational-methods",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html#computational-methods",
    "title": "Computational Methods for Drug Repositioning",
    "section": "Computational Methods",
    "text": "Computational Methods\nVirtual screening is a computational approach for predicting potentially bioactive compounds when scanning a library of small molecules [2]. The authors of this paper utilised this approach in an attempt to discover small molecule inhibitors of main protease (mPRO)[3] . They utilised a form of virtual screening that is receptor based, specifically they used protein-ligand docking in order to identify drug molecules that had a potential inhibitory effect on mPRO [5].\n\nThis technique utilises a crystallised structure of a protein in order to predict how the compounds within the virtual library would bind to the binding site of a protein[2]. With this, they analysed more than 50 crystal structures of main protease in both apo and holo forms to select a structure that would perform optimally in a molecular docking study. Ultimately selecting a holo crystallised structure of the SARS-CoV-2 main protease as it was determined that the presence of a ligand bound to the crystal structure would position the active site residues in such a manner that would be conducive to optimal performance of the molecular docking study[3].\nAs part of this technique, the protein used for the basis of docking must be prepared. As this is an experimentally obtained structure, there will be aberrations that must be corrected to ensure optimal performance of the docking protocol [6]. X-ray crystallographic protein structures have common issues such as missing hydrogen atoms, missing residues, incomplete sidechains, undefined protonation states and/or presence of crystallization products that are not found in reality[2]. In order to mitigate these issues the authors of this paper utilised the program, Reduce, on the crystal structure in order to perform alterations such as side-chain flips, optimisation of hydrogen bonds and the addition or removal of hydrogen bonds[7]. To validate that this structure was chemically sound, a further program, UCSF Chimera, was used to visually analyse the structure [[5]][8]. The next stage of the virtual screening protocol was docking. Generally, docking programs generate an initial set of conformational, tautomeric and protonation states for each of the ligand compounds[5]. Then, researchers can apply a search algorithm and scoring function to generate and score the poses of the ligand within the binding site of the receptor in an effort to identify the docked pose that likely represents the real binding mode of the compound [[9]][5]. Researchers have the autonomy to introduce constraints to these processes to assist these protocols in producing results that make chemical sense. For instance, the cavity of the protein in which compounds can be docked is commonly constrained in order to limit the space that can be occupied by docked poses [10]. Additionally, constraints regarding the specific binding modes of the docked poses can be implemented to limit the conformational space the search algorithm can select docked poses from[11].\n\n\n\nGlide docking pose of a ligand binding to Main Protease [12]\n\n\nThe authors of this paper ran four independent docking protocols within this study using Glide SP, AutoDock Vina and two protocols utilising AutoDock 4.2[5]. The Glide software package utilises a set of hierarchical filters to search for probable locations of the ligand within the binding site region of the receptor [13]. The receptors shape and properties are represented using a grid via the use of varying sets of fields that yield a progressively more accurate scoring of the ligand pose[13]. Here, the creators of the Glide software package define the term ‘pose’ to refer to a complete specification of the ligand in terms of its position and orientation in space relative to the receptor, including its core and rotamer group conformations [13]. Following the field generation, generation of initial ligand conformations is performed. This set of conformations are selected from a listing of the minima energy structures within the ligand dihedral angle space [13]. Utilising these generated conformations, an initial screening procedure is performed to identify promising poses, this pre-screening step reduces the region where more computationally expensive evaluations are performed thereby allowing for acceptable computational speed[13]. Beginning with the promising poses identified from this initial screening, the ligands are the minimised within the field of the receptor via the use of a standard molecular mechanics energy function in order to select for a smaller sub-selection of low energy poses [13]. To finally predict the binding affinity and order the screened ligands, a combination of ‘GlideScore’, the ligand-receptor molecular mechanics energy and the ligand strain energy is utilised in order to identify correctly docked poses [13]."
  },
  {
    "objectID": "posts/Computational Methods for Drug-Repositioning/index.html#references",
    "href": "posts/Computational Methods for Drug-Repositioning/index.html#references",
    "title": "Computational Methods for Drug Repositioning",
    "section": "References",
    "text": "References\n\nB. M. Sahoo, B. V. V. Ravi Kumar, J. Sruti, M. K. Mahapatra, B. K. Banik and P. Borah, Frontiers in Molecular Biosciences 2021, 8.\n\n\n\nA. Gimeno, M. Ojeda-Montes, S. Tomás-Hernández, A. Cereto-Massagué, R. Beltrán-Debón, M. Mulero, G. Pujadas and S. Garcia-Vallvé, International Journal of Molecular Sciences 2019, 20, 1375.\nM. M. Ghahremanpour, J. Tirado-Rives, M. Deshmukh, J. A. Ippolito, C.-H. Zhang, I. Cabeza De Vaca, M.-E. Liosi, K. S. Anderson and W. L. Jorgensen, ACS Medicinal Chemistry Letters 2020, 11, 2526-2533.\nZ. Jin, X. Du, Y. Xu, Y. Deng, M. Liu, Y. Zhao, B. Zhang, X. Li, L. Zhang, C. Peng, Y. Duan, J. Yu, L. Wang, K. Yang, F. Liu, R. Jiang, X. Yang, T. You, X. Liu, X. Yang, F. Bai, H. Liu, X. Liu, L. W. Guddat, W. Xu, G. Xiao, C. Qin, Z. Shi, H. Jiang, Z. Rao and H. Yang, Nature 2020, 582, 289-293.\nQ. Wang, Y. Guo, S. Iketani, M. S. Nair, Z. Li, H. Mohri, M. Wang, J. Yu, A. D. Bowen, J. Y. Chang, J. G. Shah, N. Nguyen, Z. Chen, K. Meyers, M. T. Yin, M. E. Sobieszczyk, Z. Sheng, Y. Huang, L. Liu and D. D. Ho, Nature 2022, 608, 603-608.\nE. F. Pettersen, T. D. Goddard, C. C. Huang, G. S. Couch, D. M. Greenblatt, E. C. Meng and T. E. Ferrin, Journal of Computational Chemistry 2004, 25, 1605-1612.\nR. A. Friesner, J. L. Banks, R. B. Murphy, T. A. Halgren, J. J. Klicic, D. T. Mainz, M. P. Repasky, E. H. Knoll, M. Shelley, J. K. Perry, D. E. Shaw, P. Francis and P. S. Shenkin, Journal of Medicinal Chemistry 2004, 47, 1739-1749."
  },
  {
    "objectID": "tidyr.html",
    "href": "tidyr.html",
    "title": "dplyr and tidyr",
    "section": "",
    "text": "Ways in which we can access information\n\nExtracting existing variables: select()\nExtract existing observations: filter()\nDrive new variables from existing ones: mutate()\nChange the unit of analysis: summarise()\n\nselect()\nThis function allows for the subsection of a smaller number of columns from a larger dataframe.\n\nlibrary(EDAWR)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:EDAWR':\n\n    storms\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\n\nAttaching package: 'tidyr'\n\n\nThe following objects are masked from 'package:EDAWR':\n\n    population, who\n\nlibrary(knitr)\ndf = storms\nsubset = select(df, name, pressure)\nkable((head(subset)))\n\n\n\n\nname\npressure\n\n\n\n\nAmy\n1013\n\n\nAmy\n1013\n\n\nAmy\n1013\n\n\nAmy\n1013\n\n\nAmy\n1012\n\n\nAmy\n1012\n\n\n\n\n\nDropping a column.\n\nselect(storms, -name)\n\n# A tibble: 11,859 × 12\n    year month   day  hour   lat  long status      categ…¹  wind press…² tropi…³\n   <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>       <ord>   <int>   <int>   <int>\n 1  1975     6    27     0  27.5 -79   tropical d… -1         25    1013      NA\n 2  1975     6    27     6  28.5 -79   tropical d… -1         25    1013      NA\n 3  1975     6    27    12  29.5 -79   tropical d… -1         25    1013      NA\n 4  1975     6    27    18  30.5 -79   tropical d… -1         25    1013      NA\n 5  1975     6    28     0  31.5 -78.8 tropical d… -1         25    1012      NA\n 6  1975     6    28     6  32.4 -78.7 tropical d… -1         25    1012      NA\n 7  1975     6    28    12  33.3 -78   tropical d… -1         25    1011      NA\n 8  1975     6    28    18  34   -77   tropical d… -1         30    1006      NA\n 9  1975     6    29     0  34.4 -75.8 tropical s… 0          35    1004      NA\n10  1975     6    29     6  34   -74.8 tropical s… 0          40    1002      NA\n# … with 11,849 more rows, 1 more variable: hurricane_force_diameter <int>, and\n#   abbreviated variable names ¹​category, ²​pressure,\n#   ³​tropicalstorm_force_diameter\n\n\nObtaining a range of variables.\n\nselect(storms, wind:year)\n\n# A tibble: 11,859 × 9\n    wind category status               long   lat  hour   day month  year\n   <int> <ord>    <chr>               <dbl> <dbl> <dbl> <int> <dbl> <dbl>\n 1    25 -1       tropical depression -79    27.5     0    27     6  1975\n 2    25 -1       tropical depression -79    28.5     6    27     6  1975\n 3    25 -1       tropical depression -79    29.5    12    27     6  1975\n 4    25 -1       tropical depression -79    30.5    18    27     6  1975\n 5    25 -1       tropical depression -78.8  31.5     0    28     6  1975\n 6    25 -1       tropical depression -78.7  32.4     6    28     6  1975\n 7    25 -1       tropical depression -78    33.3    12    28     6  1975\n 8    30 -1       tropical depression -77    34      18    28     6  1975\n 9    35 0        tropical storm      -75.8  34.4     0    29     6  1975\n10    40 0        tropical storm      -74.8  34       6    29     6  1975\n# … with 11,849 more rows\n\n\nfilter()\nFilter out observations using logical tests.\n\nfilter(storms, wind >= 50, name %in% c('Alberto', 'Alex', 'Allison'))\n\n# A tibble: 128 × 13\n   name     year month   day  hour   lat  long status      categ…¹  wind press…²\n   <chr>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>       <ord>   <int>   <int>\n 1 Alberto  1982     6     3    12  23.2 -84.2 tropical s… 0          50     995\n 2 Alberto  1982     6     3    18  24   -83.6 hurricane   1          75     985\n 3 Alberto  1982     6     4     0  24.8 -83.4 hurricane   1          65     992\n 4 Alberto  1982     6     4     6  24.9 -84.1 tropical s… 0          55     998\n 5 Alberto  1994     7     3     6  28.8 -86.8 tropical s… 0          50     997\n 6 Alberto  1994     7     3    12  29.9 -86.7 tropical s… 0          55     993\n 7 Alberto  1994     7     3    15  30.4 -86.5 tropical s… 0          55     993\n 8 Allison  1995     6     4     0  22   -86   tropical s… 0          50     997\n 9 Allison  1995     6     4     6  23.3 -86.3 tropical s… 0          60     995\n10 Allison  1995     6     4    12  24.7 -86.2 hurricane   1          65     987\n# … with 118 more rows, 2 more variables: tropicalstorm_force_diameter <int>,\n#   hurricane_force_diameter <int>, and abbreviated variable names ¹​category,\n#   ²​pressure\n\n\nmutate()\nMutate() allows you to generate new variables from existing ones.\n\ndf = mutate(storms, ratio = pressure/wind)\nselect(df, name, ratio)\n\n# A tibble: 11,859 × 2\n   name  ratio\n   <chr> <dbl>\n 1 Amy    40.5\n 2 Amy    40.5\n 3 Amy    40.5\n 4 Amy    40.5\n 5 Amy    40.5\n 6 Amy    40.5\n 7 Amy    40.4\n 8 Amy    33.5\n 9 Amy    28.7\n10 Amy    25.0\n# … with 11,849 more rows\n\n\nsummarise()\nCalculates summary statistics from a dataframe.\n\npollution %>% summarise(median = median(amount), variance = var(amount))\n\n  median variance\n1   22.5   1731.6\n\n\narrange()\nArranges the order of the rows within a dataframe based on the values within a column.\n\narrange(storms, wind)\n\n# A tibble: 11,859 × 13\n   name       year month   day  hour   lat  long status    categ…¹  wind press…²\n   <chr>     <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>     <ord>   <int>   <int>\n 1 Bonnie     1986     6    28     6  36.5 -91.3 tropical… -1         10    1013\n 2 Bonnie     1986     6    28    12  37.2 -90   tropical… -1         10    1012\n 3 AL031987   1987     8    16    18  30.9 -83.2 tropical… -1         10    1014\n 4 AL031987   1987     8    17     0  31.4 -82.9 tropical… -1         10    1015\n 5 AL031987   1987     8    17     6  31.8 -82.3 tropical… -1         10    1015\n 6 Alberto    1994     7     7     0  32.7 -86.3 tropical… -1         10    1012\n 7 Alberto    1994     7     7     6  32.7 -86.6 tropical… -1         10    1012\n 8 Alberto    1994     7     7    12  32.8 -86.8 tropical… -1         10    1012\n 9 Alberto    1994     7     7    18  33   -87   tropical… -1         10    1013\n10 Claudette  1979     7    27    12  34   -95.9 tropical… -1         15    1007\n# … with 11,849 more rows, 2 more variables:\n#   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n#   abbreviated variable names ¹​category, ²​pressure\n\narrange(storms, desc(wind))\n\n# A tibble: 11,859 × 13\n   name     year month   day  hour   lat  long status    category  wind pressure\n   <chr>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>     <ord>    <int>    <int>\n 1 Gilbert  1988     9    14     0  19.7 -83.8 hurricane 5          160      888\n 2 Wilma    2005    10    19    12  17.3 -82.8 hurricane 5          160      882\n 3 Dorian   2019     9     1    16  26.5 -77   hurricane 5          160      910\n 4 Dorian   2019     9     1    18  26.5 -77.1 hurricane 5          160      910\n 5 Gilbert  1988     9    14     6  19.9 -85.3 hurricane 5          155      889\n 6 Mitch    1998    10    26    18  16.9 -83.1 hurricane 5          155      905\n 7 Mitch    1998    10    27     0  17.2 -83.8 hurricane 5          155      910\n 8 Rita     2005     9    22     3  24.7 -87.3 hurricane 5          155      895\n 9 Rita     2005     9    22     6  24.8 -87.6 hurricane 5          155      897\n10 Dorian   2019     9     1    12  26.5 -76.5 hurricane 5          155      927\n# … with 11,849 more rows, and 2 more variables:\n#   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>\n\n\nAdding additional variables into the arrange function will use those as tiebreakers between observations that have the same value.\n\narrange(storms, wind,year)\n\n# A tibble: 11,859 × 13\n   name       year month   day  hour   lat  long status    categ…¹  wind press…²\n   <chr>     <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>     <ord>   <int>   <int>\n 1 Bonnie     1986     6    28     6  36.5 -91.3 tropical… -1         10    1013\n 2 Bonnie     1986     6    28    12  37.2 -90   tropical… -1         10    1012\n 3 AL031987   1987     8    16    18  30.9 -83.2 tropical… -1         10    1014\n 4 AL031987   1987     8    17     0  31.4 -82.9 tropical… -1         10    1015\n 5 AL031987   1987     8    17     6  31.8 -82.3 tropical… -1         10    1015\n 6 Alberto    1994     7     7     0  32.7 -86.3 tropical… -1         10    1012\n 7 Alberto    1994     7     7     6  32.7 -86.6 tropical… -1         10    1012\n 8 Alberto    1994     7     7    12  32.8 -86.8 tropical… -1         10    1012\n 9 Alberto    1994     7     7    18  33   -87   tropical… -1         10    1013\n10 Claudette  1979     7    27    12  34   -95.9 tropical… -1         15    1007\n# … with 11,849 more rows, 2 more variables:\n#   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n#   abbreviated variable names ¹​category, ²​pressure\n\n\nPipe Operator\n\nstorms %>%\n  filter(wind>=50) %>%\n  select(name, pressure)\n\n# A tibble: 5,756 × 2\n   name  pressure\n   <chr>    <int>\n 1 Amy        998\n 2 Amy        998\n 3 Amy        998\n 4 Amy        987\n 5 Amy        987\n 6 Amy        984\n 7 Amy        984\n 8 Amy        984\n 9 Amy        984\n10 Amy        984\n# … with 5,746 more rows\n\n\n\nstorms %>%\n  mutate(ratio = pressure/wind) %>%\n  select(name, ratio) \n\n# A tibble: 11,859 × 2\n   name  ratio\n   <chr> <dbl>\n 1 Amy    40.5\n 2 Amy    40.5\n 3 Amy    40.5\n 4 Amy    40.5\n 5 Amy    40.5\n 6 Amy    40.5\n 7 Amy    40.4\n 8 Amy    33.5\n 9 Amy    28.7\n10 Amy    25.0\n# … with 11,849 more rows\n\n\ngroup_by() + summarise()\n\ngrouped_pollution = pollution %>% group_by(city) %>% summarise(mean = mean(amount), sum = sum(amount), n = n())\nkable(grouped_pollution)\n\n\n\n\ncity\nmean\nsum\nn\n\n\n\n\nBeijing\n88.5\n177\n2\n\n\nLondon\n19.0\n38\n2\n\n\nNew York\n18.5\n37\n2\n\n\n\n\n\n\nlibrary(gtsummary)\ngrouped_pollution %>% select(city,mean,sum) %>% tbl_summary()\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N = 31\n    \n  \n  \n    city\n\n        Beijing\n1 (33%)\n        London\n1 (33%)\n        New York\n1 (33%)\n    mean\n\n        18.5\n1 (33%)\n        19\n1 (33%)\n        88.5\n1 (33%)\n    sum\n\n        37\n1 (33%)\n        38\n1 (33%)\n        177\n1 (33%)\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "posts/Ring Opening Reaction of N-Acetylisatin/index.html",
    "href": "posts/Ring Opening Reaction of N-Acetylisatin/index.html",
    "title": "Ring Opening Reaction of N-Acetylisatin",
    "section": "",
    "text": "This article describes a four week project based on developing peptidomimetics."
  },
  {
    "objectID": "posts/Neuromatch deep learning lesson 1/index.html",
    "href": "posts/Neuromatch deep learning lesson 1/index.html",
    "title": "Basics of Pytorch",
    "section": "",
    "text": "Pytorch can be used for the direct construction of tensors in the following manner.\n\n# From a list\na = torch.tensor([0,1,2])\n\n# From a tuple of tuples\nb = ((1.0,1.1), (1.2,1.3))\nb = torch.tensor(b)\n\n# From a numpy array\nc = np.ones([2,3])\nc = torch.tensor(c)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Tensor b: {b}\")\nprint(f\"Tensor c: {c}\")\n\nTensor a: tensor([0, 1, 2])\nTensor b: tensor([[1.0000, 1.1000],\n        [1.2000, 1.3000]])\nTensor c: tensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\n\n\nIt is also possible to create tensors from various forms of random number distributions.\n\n# Uniform distribution\na = torch.rand(1, 3)\n\n# Normal distribution\nb = torch.randn(3, 4)\n\n# There are also constructors that allow us to construct\n# a tensor according to the above constructors, but with\n# dimensions equal to another tensor.\n\nc = torch.zeros_like(a)\nd = torch.rand_like(c)\n\nprint(f\"Tensor a: {a}\")\nprint(f\"Tensor b: {b}\")\nprint(f\"Tensor c: {c}\")\nprint(f\"Tensor d: {d}\")\n\nTensor a: tensor([[0.6242, 0.3680, 0.1774]])\nTensor b: tensor([[-1.4357, -0.3610, -0.4312, -1.1694],\n        [ 1.0250,  0.8717,  1.0414, -0.2386],\n        [-0.8637,  1.2593,  0.0024,  0.6281]])\nTensor c: tensor([[0., 0., 0.]])\nTensor d: tensor([[0.6016, 0.5186, 0.7333]])\n\n\n\nimport torch\ntorch.manual_seed(0)\n\nimport random\nrandom.seed(0)\n\nimport numpy as np\nnp.random.seed(0)\n\nseed = 10\n\n\ndef set_seed(seed=None, seed_torch=True):\n  \"\"\"\n  Function that controls randomness. NumPy and random modules must be imported.\n\n  Args:\n    seed : Integer\n      A non-negative integer that defines the random state. Default is `None`.\n    seed_torch : Boolean\n      If `True` sets the random seed for pytorch tensors, so pytorch module\n      must be imported. Default is `True`.\n\n  Returns:\n    Nothing.\n  \"\"\"\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\ndef simplefun(seed=True, my_seed=None):\n  \"\"\"\n  Helper function to verify effectiveness of set_seed attribute\n\n  Args:\n    seed: Boolean\n      Specifies if seed value is provided or not\n    my_seed: Integer\n      Initializes seed to specified value\n\n  Returns:\n    Nothing\n  \"\"\"\n  if seed:\n    set_seed(seed=my_seed)\n\n  # uniform distribution\n  a = torch.rand(1, 3)\n  # normal distribution\n  b = torch.randn(3, 4)\n\n  print(\"Tensor a: \", a)\n  print(\"Tensor b: \", b)\n\n\nsimplefun(seed=True, my_seed=0)  # Turn `seed` to `False` or change `my_seed`\n\nRandom seed 0 has been set.\nTensor a:  tensor([[0.4963, 0.7682, 0.0885]])\nTensor b:  tensor([[ 0.3643,  0.1344,  0.1642,  0.3058],\n        [ 0.2100,  0.9056,  0.6035,  0.8110],\n        [-0.0451,  0.8797,  1.0482, -0.0445]])\n\n\nPytorch also provides equivalents of numpy .linscape() and .arrange() functions which operate as you would expect.\n\na = torch.arange(0, 10, step=1)\nb = np.arange(0, 10, step=1)\n\nc = torch.linspace(0, 5, steps=11)\nd = np.linspace(0, 5, num=11)\n\nprint(f\"Tensor a: {a}\\n\")\nprint(f\"Numpy array b: {b}\\n\")\nprint(f\"Tensor c: {c}\\n\")\nprint(f\"Numpy array d: {d}\\n\")\n\nTensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nNumpy array b: [0 1 2 3 4 5 6 7 8 9]\n\nTensor c: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n        4.5000, 5.0000])\n\nNumpy array d: [0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5. ]\n\n\n\n\ndef tensor_creation(Z):\n  \"\"\"\n  A function that creates various tensors.\n\n  Args:\n    Z: numpy.ndarray\n      An array of shape (3,4)\n\n  Returns:\n    A : Tensor\n      20 by 21 tensor consisting of ones\n    B : Tensor\n      A tensor with elements equal to the elements of numpy array Z\n    C : Tensor\n      A tensor with the same number of elements as A but with values ∼U(0,1)\n    D : Tensor\n      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n  \"\"\"\n  #################################################\n  ## TODO for students: fill in the missing code\n  ## from the first expression\n  \n  #################################################\n  A = torch.ones(20,21)\n  B = torch.tensor(Z)\n  C = torch.randn(20,21)\n  D = torch.linspace(4,41, steps=2)\n  \n  return A, B, C, D\n\n\n# numpy array to copy later\nZ = np.vander([1, 2, 3], 4)\n\n# Uncomment below to check your function!\nA, B, C, D = tensor_creation(Z)\n\n\na = torch.ones(5, 3)\nb = torch.rand(5, 3)\nc = torch.empty(5, 3)\nd = torch.empty(5, 3)\n\n# this only works if c and d already exist\ntorch.add(a, b, out=c)\n\n# Pointwise Multiplication of a and b\ntorch.multiply(a, b, out=d)\n\nprint(c)\nprint(d)\n\ntensor([[1.1074, 1.6594, 1.7684],\n        [1.5697, 1.1655, 1.1123],\n        [1.3457, 1.7195, 1.9932],\n        [1.7875, 1.4437, 1.6753],\n        [1.0095, 1.0729, 1.7333]])\ntensor([[0.1074, 0.6594, 0.7684],\n        [0.5697, 0.1655, 0.1123],\n        [0.3457, 0.7195, 0.9932],\n        [0.7875, 0.4437, 0.6753],\n        [0.0095, 0.0729, 0.7333]])\n\n\n\ndef simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n  \"\"\"\n  Helper function to demonstrate simple operations\n  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n\n  Args:\n    a1: Torch tensor\n      Tensor of size ([2,2])\n    a2: Torch tensor\n      Tensor of size ([2,2])\n    a3: Torch tensor\n      Tensor of size ([2,2])\n\n  Returns:\n    answer: Torch tensor\n      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n  \"\"\"\n  ################################################\n  ## TODO for students:  complete the first computation using the argument matricies\n  \n  ################################################\n  #\n  answer = a1 @ a2 + a3 \n  return answer\n\n# add timing to airtable\n\n\n# Computing expression 1:\n\n# init our tensors\na1 = torch.tensor([[2, 4], [5, 7]])\na2 = torch.tensor([[1, 1], [2, 3]])\na3 = torch.tensor([[10, 10], [12, 1]])\n## uncomment to test your function\nA = simple_operations(a1, a2, a3)\nprint(A)\n\ntensor([[20, 24],\n        [31, 27]])\n\n\n\ndef dot_product(b1: torch.Tensor, b2: torch.Tensor):\n  ###############################################\n  ## TODO for students:  complete the first computation using the argument matricies\n  \n  ###############################################\n  \"\"\"\n  Helper function to demonstrate dot product operation\n  Dot product is an algebraic operation that takes two equal-length sequences\n  (usually coordinate vectors), and returns a single number.\n  Geometrically, it is the product of the Euclidean magnitudes of the\n  two vectors and the cosine of the angle between them.\n\n  Args:\n    b1: Torch tensor\n      Tensor of size ([3])\n    b2: Torch tensor\n      Tensor of size ([3])\n\n  Returns:\n    product: Tensor\n      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n  \"\"\"\n  # Use torch.dot() to compute the dot product of two tensors\n  product = torch.dot(b1, b2)\n  return product\n\n# add timing to airtable\n\n\n# Computing expression 2:\nb1 = torch.tensor([3, 5, 7])\nb2 = torch.tensor([2, 4, 8])\n## Uncomment to test your function\nb = dot_product(b1, b2)\nprint(b)\n\ntensor(82)\n\n\nIrrelevant dimensions\nThere is a command .squeeze that gets rid of the singleton dimension it allow for indexing into the first dimension of a rank 1 tensor more simply.\n\nx = torch.randn(1,10)\nprint(x.shape)\nprint(\"x[0]: \", x[0])\n\nx = x.squeeze(0)\nprint(x.shape)\nprint(\"x[0]: \", x[0])\n\ntorch.Size([1, 10])\nx[0]:  tensor([-0.0612, -0.6197, -0.3718, -1.2872, -0.5185,  0.0805,  1.9483,  2.0819,\n        -0.9010,  1.1303])\ntorch.Size([10])\nx[0]:  tensor(-0.0612)"
  },
  {
    "objectID": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#methods",
    "href": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#methods",
    "title": "Ring Opening Reaction of N-Acetylisatin",
    "section": "Methods",
    "text": "Methods\nN-acetylisatin (0.5 g) and dichloromethane (2 mL) were combined in a round bottom flask before the drop-wise addition of pyrrolidine (0.22 mL, 2.64 mmol) whilst the reaction mixture was stirred. This reaction mixture was then refluxed for two hours and the reaction progress was monitored by TLC. Following this, the reaction mixture was allowed to cool to room temperature prior to being transferred to a separatory funnel for a liquid-liquid extraction with hydrochloric acid (15 mL, 2M). The organic dichloromethane layer was retained whilst the aqueous layer was discarded. This organic layer was washed with brine before another liquid-liquid extraction was performed. Once again, the organic layer was retained and magnesium sulfate was added to this aliquot and the mixture was then filtered. The solvent was then removed using a rotary evaporator. The product of this reaction is an oil."
  },
  {
    "objectID": "posts/Nanoparticulate Drug Delivery Systems/index.html",
    "href": "posts/Nanoparticulate Drug Delivery Systems/index.html",
    "title": "Nanoparticulate Drug Delivery Systems",
    "section": "",
    "text": "Why are nanoparticles used in medicine?\nNanoparticles used for medicinal purposes are referred to as nanomedicines and these agents have multi-dimensional usage as diagnostic tools or vehicles used for the targeted delivery of selected therapeutic compounds(Sadeghi et al. 2020). Nanoparticles are defined as being between 1-100 nanometres (nm) in diameter and these particles have already been used in the delivery of drugs such as vaccines, nucleotides and recombinant proteins(Sadeghi et al. 2020). These nanoparticle dependent delivery strategies exhibit improvements in rate of absorption, reduction in elimination kinetics and controlled release(Sadeghi et al. 2020).\nTraditional drug administration normally involves the oral and intravenous delivery of drug molecules. These drug particles travel in an unguided manner and this results in a systemic distribution throughout the body(Lu et al. 2021). It is instead desirable to have the drug molecules primarily be located at the desired site of action as this systemic distribution results in low drug efficacy and enhances the potential for off target effects(Lu et al. 2021).\nAn important consideration within pharmacological treatments is how to best maintain the concentration of the drug at the site of action. The optimal concentration range is termed as the therapeutic window and this window is a range in which the effectiveness of the drug is maintained without reaching toxic levels. The use of nanoparticle delivery systems allows for the pharmacokinetics and biodistribution of a drug to be dependent upon the characteristics of a given nanoparticle and not upon the drug molecule.\nNanoparticles have been constructed from various materials and the four major compositional categories are: organic, inorganic, carbon-based and composite-based(Jeevanandam et al. 2018). The material choice in addition to the size and shape of the nanoparticle are key in determining its biological outcomes(Jeevanandam et al. 2018). Additional considerations are the particles surface bioactivity and functionality.\n\n\n\n\n\nNanoparticle composition categories (Sadeghi et al. 2020)\n\n\n\n\nBiodistribution of nanoparticles\nNanoparticles can be administered using various different routes (see table 1), however the preeminent manner is intravenous administration currently. The route of administration plays a large role in determining the biodistribution of a nanoparticle and specific characteristics of the particle can be tuned in order to have further control over the specific site of deposition for a given particle. When nanoparticles are inhaled, there is significant accumulation in the lung tissue, however the diameter of the nanoparticle can critically determine the specific site the particles will accumulate. Figure 1 demonstrates that particles (0.001-0.01 \\(\\mu\\)m) can be seen to accumulate significantly in the extra thoracic region whereas relatively larger particles begin to accumulate more significantly in the intrathoracic region(Geiser and Kreyling 2010).\n\n\n\nFigure 1: Particle deposition in the respiratory tract.\n\n\n\nTable 1. Comparison of routes of administration(Sadeghi et al. 2020)\n\n\n\n\n\n\n\nRoute\nAdvantages\nDisadvantages/Challenges for nanoparticles\n\n\n\n\nOral\n\nPatient compliance\nFewer infections/contamination\nDosage flexibility\nExtensive drug absorption area in the GI tract\n\n\nAcidic pH of the stomach\nPresence of proteolytic enzymes in the GI tract\nLow permeation across the intestinal epithelium\n\n\n\nNasal\n\nLarge surface area particularly in the lungs\nHighly vascularised mucosa\nPorous endothelial membrane\nNegligible enzymatic activity\nBypasses first-pass metabolism\n\n\nSize limited through the delivery device\nPresence of nasal clearance\nProteolytic instability\n\n\n\nPulmonary\n\nLarge absorption surface area\nSignificant vascularisation\nNarrow alveolar epithelial membrane\nNegligible enzymatic activity\n\n\nCan have an irritant effect on the airways\nProtein and peptide degradation\nLow retention of drug within the lungs\n\n\n\nIntravenous\n\nSystemic delivery\nSystemic action\n\n\nInvasive\nHeptatotoxicity\nSystemic toxicity\n\n\n\nOcular\n\nEase of administration\nCircumvents first-pass metabolism\n\n\nPoor permeability\nEnzymatic degradation\nNasolacrimal drainage\n\n\n\n\n\n\n\n\n\n\n\n\nFrom I.V administration to the cell\n\nGeiser, Marianne, and Wolfgang G Kreyling. 2010. “Deposition and Biokinetics of Inhaled Nanoparticles.” Particle and Fibre Toxicology 7 (1): 2. https://doi.org/10.1186/1743-8977-7-2.\n\n\nJeevanandam, Jaison, Ahmed Barhoum, Yen S Chan, Alain Dufresne, and Michael K Danquah. 2018. “Review on Nanoparticles and Nanostructured Materials: History, Sources, Toxicity and Regulations.” Beilstein Journal of Nanotechnology 9 (April): 1050–74. https://doi.org/10.3762/bjnano.9.98.\n\n\nLu, Weijia, Jing Yao, Xiao Zhu, and Yi Qi. 2021. “Nanomedicines: Redefining Traditional Medicine.” Biomedicine & Pharmacotherapy 134 (February): 111103. https://doi.org/10.1016/j.biopha.2020.111103.\n\n\nSadeghi, Samira, Wai Kit Lee, Shik Nie Kong, Annanya Shetty, and Chester Lee Drum. 2020. “Oral Administration of Protein Nanoparticles: An Emerging Route to Disease Treatment.” Pharmacological Research 158 (August): 104685. https://doi.org/10.1016/j.phrs.2020.104685."
  },
  {
    "objectID": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#ring-opening-reaction-with-glycine-methyl-ester",
    "href": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#ring-opening-reaction-with-glycine-methyl-ester",
    "title": "Ring Opening Reaction of N-Acetylisatin",
    "section": "Ring opening reaction with glycine methyl ester",
    "text": "Ring opening reaction with glycine methyl ester\nA solution of glycine methyl ester hydrochloride (0.286 g, 3.17 mmol), acetonitrile (7.5 mL) was prepared and stirred before triethylamine (0.5 mL, 3.59 mmol) was added dropwise to the solution. To this solution, N-acetylisatin (0.5 g, 264 mmol) was then added and the reaction was left to run under vacuum whilst stirring. The reaction progress was monitored via thin layer chromatography and once complete the reaction mixed was evaporated to dryness using a rotary evaporator.\n\n\n\n\n\nFigure 3: Ring opening reaction scheme"
  },
  {
    "objectID": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#synthesis-of-n-acetylisatin",
    "href": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#synthesis-of-n-acetylisatin",
    "title": "Ring Opening Reaction of N-Acetylisatin",
    "section": "Synthesis of N-Acetylisatin",
    "text": "Synthesis of N-Acetylisatin\nA solution of Isatin (3.12 g, 20.4 mmol) and Acetic anhydride (15 mL, 0.159 mol) was prepared and refluxed for two hours and 15 minutes at 140 \\(^{\\circ}\\)C. The reaction mixture was then filtered and washed with hexane to yield the yellow crystalline product, N-acetylisatin (2.98 g, 74%).\n\n\n\n\n\nFigure 1: Synthesis of N-Acetylisatin\n\n\nAn alternative synthetic procedure for the synthesis of N-acetylisatin from isatin where acetic anhydride is not used is provided by Chen, Qifa et al(Chen, Teng, and Xu 2021). A solution of isatin (2.94 g), alkyl bromide (2.40 g), K2CO3 (4.14g, 30 mmol) in MeCN (40 mL) is to be prepared. This mixture is then stirred at reflux for eight hours and when the reaction is complete the solvent is evaporated away under vacuum. The crude product can then be purified using column chromatography on silica gel with an ethyl acetate and petroleum ether eluent system."
  },
  {
    "objectID": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#esterification-of-glycine",
    "href": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#esterification-of-glycine",
    "title": "Ring Opening Reaction of N-Acetylisatin",
    "section": "Esterification of glycine",
    "text": "Esterification of glycine\nA solution of glycine (1.124 g), TMSCl (10 mL) and MeOH (three equivalents) was prepared and kept in an air tight environment before being left to stir for three hours. After the reaction was complete (monitored via TLC), the reaction mixture was concentrated on a rotary evaporator to yield a white crystalline solid, glycine methyl ester (1.5 g)"
  },
  {
    "objectID": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#ring-opening-reaction-with-butylamine",
    "href": "posts/Ring Opening Reaction of N-Acetylisatin/index.html#ring-opening-reaction-with-butylamine",
    "title": "Ring Opening Reaction of N-Acetylisatin",
    "section": "Ring opening reaction with butylamine",
    "text": "Ring opening reaction with butylamine\n\nMethods\nN-acetylisatin (0.5 g) and dichloromethane (2 mL) were combined in a round bottom flask before the drop-wise addition of butylamine (0.26 mL, 2.64 mmol) whilst the reaction mixture was stirred. This reaction mixture was then refluxed for two hours and the reaction progress was monitored by TLC. Following this, the reaction mixture was allowed to cool to room temperature prior to being transferred to a separatory funnel for a liquid-liquid extraction with hydrochloric acid (15 mL, 2M). The organic dichloromethane layer was retained whilst the aqueous layer was discarded. This organic layer was washed with brine before another liquid-liquid extraction was performed. Once again, the organic layer was retained and magnesium sulfate was added to this aliquot and the mixture was then filtered. The solvent was then removed using a rotary evaporator. The product of this reaction was a fine yellow solid (0.4 g).\n\n\n\n\n\nFigure 2: Ring opening reaction with butylamine"
  },
  {
    "objectID": "posts/Tidymodels Tutorial/index.html",
    "href": "posts/Tidymodels Tutorial/index.html",
    "title": "Tidymodels in R",
    "section": "",
    "text": "Building a model\n\n\n\n\n\n\n\nknitr::kable(head(urchins))\n\n\n\n\nfood_regime\ninitial_volume\nwidth\n\n\n\n\nInitial\n3.5\n0.010\n\n\nInitial\n5.0\n0.020\n\n\nInitial\n8.0\n0.061\n\n\nInitial\n10.0\n0.051\n\n\nInitial\n13.0\n0.041\n\n\nInitial\n13.0\n0.061\n\n\n\n\n\n\nggplot(urchins,\n       aes(x = initial_volume, \n           y = width, \n           group = food_regime, \n           col = food_regime)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n#> `geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "lol.html",
    "href": "lol.html",
    "title": "Computational Methods for Drug Repositioning",
    "section": "",
    "text": "This paper outlines the use of computational chemistry methods to identify approved drugs that display an inhibitory effect against the main protease of SARS-CoV-2. This process is known as drug repurposing but is also termed as drug repositioning or therapeutic switching. This approach is used to identify novel therapeutic agents from a pool of existing FDA approved drug molecules. This approach is highly relevant as the drug discovery process is expensive, time-consuming, and financially risky. Thus, drug repositioning is employed to increase the success rate of drug development by offering numerous advantages relative to tradition drug discovery processes. These include a substantial reduction in the duration of the drug development process, lower-cost and higher efficiency and a significantly lower risk of failure[1]. The COVID-19 global pandemic highlighted an urgent need to develop effective therapeutic agents in a much shorter timeframe than traditional drug discovery processes allow for, thus drug repositioning efforts have been explored thoroughly as part of the effort to identify drug molecules for the treatment of COVID-19[1]. One of the predominant strategies used in drug-repurposing approaches is the use of computational methods such as molecular docking or molecular similarity. Computational methods follow along with the benefits of drug-repurposing approaches in that they allow for a time and cost efficient approach to identifying bioactive molecules to be examined for potential use as pharmacological agents[2].\nSARS-COV-2 has two proteases: chymotrypsin-like cysteine and main protease[3]. These enzymes are responsible for processing polyproteins which have been translated from viral RNA[3]. Inhibition of these proteases would result in an impediment of viral replication thereby resulting in a therapeutic effect[3]. It is also of note that no human proteases are known to share a similar cleavage specificity thus this inhibitory effect is unlikely to induce a toxic effect[3].\nThe rise of Omicron subvariants that carry mutations in their spike proteins leads to concerns regarding the efficacy of neutralising antibodies and correspondingly the efficacy of existing vaccines and therapeutic treatments[4]. Thus, the need for novel and efficacious chemotherapeutic treatments is clearly apparent. Drug repurposing efforts powered by computational chemistry methods (virtual screening/docking studies) provides a systematic path for ongoing drug discovery efforts to develop therapies effective against SARS-COV-2."
  },
  {
    "objectID": "lol.html#design",
    "href": "lol.html#design",
    "title": "Computational Methods for Drug Repositioning",
    "section": "Design",
    "text": "Design\nThe authors began by applying a consensus molecular docking protocol to scan a virtual library of approved drugs (around 2000 drugs). A high-resolution crystal structure of main protease (Figure 1) that was co-crystallized with a non-covalent small fragment hit was used as the basis ligand of this docking study[5]. This decision was made as the presence of a ligand within this crystal structure would most likely alter the conformation of the active site side chain residues in a manner that would be better for the performance of molecular docking. The molecular docking protocol of this study included four independent trials of protein-ligand docking. The docking programs Glide SP, AutoDock Vina and AutoDock 4.2 were utilised, with AutoDock 4.2 being used for two of the protocols[3]. Following this, a ranking of the compounds in order of their docking scores was conducted and the top 200 hits from each docking trial were compared. From these hits compounds that were in the top 200 in all four or at least three of the trials were selected to proceed to the next stage of the experiment. This docking protocol narrowed the library down to 42 candidates which were then extensively analyzed with respect to conformation, stability in molecular dynamics simulations and consideration of intermolecular contacts. This further narrowed down the candidates to 17, which were then examined via kinetic assays for inhibition of mPRO.\n\n\n\nRendering of the catalytic site of the crystal structure of Main Protease [3]"
  },
  {
    "objectID": "lol.html#computational-methods",
    "href": "lol.html#computational-methods",
    "title": "Computational Methods for Drug Repositioning",
    "section": "Computational Methods",
    "text": "Computational Methods\nVirtual screening is a computational approach for predicting potentially bioactive compounds when scanning a library of small molecules [2]. The authors of this paper utilised this approach in an attempt to discover small molecule inhibitors of main protease (mPRO)[3] . They utilised a form of virtual screening that is receptor based, specifically they used protein-ligand docking in order to identify drug molecules that had a potential inhibitory effect on mPRO [5].\n\nThis technique utilises a crystallised structure of a protein in order to predict how the compounds within the virtual library would bind to the binding site of a protein[2]. With this, they analysed more than 50 crystal structures of main protease in both apo and holo forms to select a structure that would perform optimally in a molecular docking study. Ultimately selecting a holo crystallised structure of the SARS-CoV-2 main protease as it was determined that the presence of a ligand bound to the crystal structure would position the active site residues in such a manner that would be conducive to optimal performance of the molecular docking study[3].\nAs part of this technique, the protein used for the basis of docking must be prepared. As this is an experimentally obtained structure, there will be aberrations that must be corrected to ensure optimal performance of the docking protocol [6]. X-ray crystallographic protein structures have common issues such as missing hydrogen atoms, missing residues, incomplete sidechains, undefined protonation states and/or presence of crystallization products that are not found in reality[2]. In order to mitigate these issues the authors of this paper utilised the program, Reduce, on the crystal structure in order to perform alterations such as side-chain flips, optimisation of hydrogen bonds and the addition or removal of hydrogen bonds[7]. To validate that this structure was chemically sound, a further program, UCSF Chimera, was used to visually analyse the structure [[5]][8]. The next stage of the virtual screening protocol was docking. Generally, docking programs generate an initial set of conformational, tautomeric and protonation states for each of the ligand compounds[5]. Then, researchers can apply a search algorithm and scoring function to generate and score the poses of the ligand within the binding site of the receptor in an effort to identify the docked pose that likely represents the real binding mode of the compound [[9]][5]. Researchers have the autonomy to introduce constraints to these processes to assist these protocols in producing results that make chemical sense. For instance, the cavity of the protein in which compounds can be docked is commonly constrained in order to limit the space that can be occupied by docked poses [10]. Additionally, constraints regarding the specific binding modes of the docked poses can be implemented to limit the conformational space the search algorithm can select docked poses from[11].\n\n\n\nGlide docking pose of a ligand binding to Main Protease [12]\n\n\nThe authors of this paper ran four independent docking protocols within this study using Glide SP, AutoDock Vina and two protocols utilising AutoDock 4.2[5]. The Glide software package utilises a set of hierarchical filters to search for probable locations of the ligand within the binding site region of the receptor [13]. The receptors shape and properties are represented using a grid via the use of varying sets of fields that yield a progressively more accurate scoring of the ligand pose[13]. Here, the creators of the Glide software package define the term ‘pose’ to refer to a complete specification of the ligand in terms of its position and orientation in space relative to the receptor, including its core and rotamer group conformations [13]. Following the field generation, generation of initial ligand conformations is performed. This set of conformations are selected from a listing of the minima energy structures within the ligand dihedral angle space [13]. Utilising these generated conformations, an initial screening procedure is performed to identify promising poses, this pre-screening step reduces the region where more computationally expensive evaluations are performed thereby allowing for acceptable computational speed[13]. Beginning with the promising poses identified from this initial screening, the ligands are the minimised within the field of the receptor via the use of a standard molecular mechanics energy function in order to select for a smaller sub-selection of low energy poses [13]. To finally predict the binding affinity and order the screened ligands, a combination of ‘GlideScore’, the ligand-receptor molecular mechanics energy and the ligand strain energy is utilised in order to identify correctly docked poses [13]."
  },
  {
    "objectID": "Untitled/Untitled.html",
    "href": "Untitled/Untitled.html",
    "title": "A template for the arxiv style",
    "section": "",
    "text": "Here goes an introduction text"
  },
  {
    "objectID": "Untitled/Untitled.html#headings-second-level",
    "href": "Untitled/Untitled.html#headings-second-level",
    "title": "A template for the arxiv style",
    "section": "Headings: second level",
    "text": "Headings: second level\nYou can use equation in blocks\n\\[\n\\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\\theta)= {\\frac {\\alpha _{i}(t)a^{w_t}_{ij}\\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\\sum _{i=1}^{N} \\sum _{j=1}^{N} \\alpha _{i}(t)a^{w_t}_{ij}\\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}\n\\]\nBut also inline i.e \\(z=x+y\\)\n\nHeadings: third level\nAnother paragraph."
  },
  {
    "objectID": "Untitled/Untitled.html#figures",
    "href": "Untitled/Untitled.html#figures",
    "title": "A template for the arxiv style",
    "section": "Figures",
    "text": "Figures\nYou can insert figure using LaTeX directly.\nSee Figure \\(\\ref{fig:fig1}\\). Here is how you add footnotes. [^Sample of the first footnote.]\nBut you can also do that using R.\n\nplot(mtcars$mpg)\n\n\n\n\nAnother sample figure\n\n\n\n\nYou can use bookdown to allow references for Tables and Figures."
  },
  {
    "objectID": "Untitled/Untitled.html#tables",
    "href": "Untitled/Untitled.html#tables",
    "title": "A template for the arxiv style",
    "section": "Tables",
    "text": "Tables\nBelow we can see how to use tables.\nSee awesome Table~\\(\\ref{tab:table}\\) which is written directly in LaTeX in source Rmd file.\nYou can also use R code for that.\n\nknitr::kable(head(mtcars), caption = \"Head of mtcars table\")\n\n\nHead of mtcars table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1"
  },
  {
    "objectID": "Untitled/Untitled.html#lists",
    "href": "Untitled/Untitled.html#lists",
    "title": "A template for the arxiv style",
    "section": "Lists",
    "text": "Lists\n\nItem 1\nItem 2\nItem 3"
  }
]